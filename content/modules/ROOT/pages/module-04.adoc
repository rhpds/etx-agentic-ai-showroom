= Using Llama Stack

[NOTE]
====
Persona: AI Engineer (primary). Also relevant: Platform Engineer.
====

[IMPORTANT]
.In this section [Needs update]
====
We are going to look at the details of LlamaStack server configuration. Along the way we will use the LlamaStack playground UI as a test client and show how to configure providers, models and MCP servers.

In the final section we will use the playground to develop our Use Case specific prompts.
====

[NOTE]
====
Estimated time: 60â€“90 minutes
====

== What you'll do

* Interact with Llama Stack & Playground
* Register built-in/MCP tools
* Develop and refine prompts in Playground
* Capture configuration and decisions for later modules


== LlamaStack

https://llama-stack.readthedocs.io/en/latest/[LlamaStack,window=_blank] is the open-source framework for building generative AI applications. We are going to deploy LlamaStack distribution and then take a look around using the client CLI and the playground UI.


=== LlamaStack Distribution

We'll be creating a LlamaStack Distribution custom resource (CR) to define how a LlamaStack server should be deployed. It allows you to specify:

* **Server Configuration**: Which distribution to use (Ollama, vLLM, etc.)
* **Container Specifications**: Port, environment variables, resource limits

There is a unique instance of LlamaStack distribution deployed for each lab user. Here is the LlamaStack Distribution CR we're using for this lab.

image::llamastackdistribution-yaml.png[LlamaStack distribution YAML, 800]

You can specify/inject the secrets for the services that'll be interacting with LlamaStack server. In our case, we've specified the secrets for [1] MaaS API and [2] Tavily search API. You also list the user configuration used with this distribution.

=== LlamaStack ConfigMap

A ConfigMap can be used to store run.yaml configuration for each LlamaStack distribution. Updates to the ConfigMap will restart the Pod to load the new data.

Example to create a run.yaml ConfigMap

[source,yaml,options="wrap"]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: llama-stack-config
  namespace: {{ .Values.username }}-llama-stack
data:
  run.yaml: |
    # Llama Stack configuration
    version: '2'
    image_name: vllm
    apis:
    - inference
    - tool_runtime
    - agents
    - safety
    - vector_io
    models:
      - metadata: {}
        model_id: {{ .Values.defaultModel.name }}
        provider_id: vllm-{{ .Values.defaultModel.name }}
        provider_model_id: {{ .Values.defaultModel.name }}
        model_type: llm
    providers:
      inference:
      - provider_id: vllm-{{ .Values.defaultModel.name }}
        provider_type: "remote::vllm"
        config:
          url: {{ .Values.defaultModel.url }}
          context_length: 4096
          api_token: ${env.DEFAULT_MODEL_API_TOKEN}
          tls_verify: true
      tool_runtime:
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_API_KEY}
          max_results: 3
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/agents_store.db
          responses_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/responses_store.db
    tools:
      - name: builtin::websearch
        enabled: true
    tool_groups:
    - provider_id: tavily-search
      toolgroup_id: builtin::websearch
    server:
      port: 8321
----


=== Interacting with LlamaStack

. Install the **llama-stack-client** - either in a notebook, or from your jumphost - ideally we match client and server versions

+
[source,bash,options="wrap",role="execute"]
----
pip install llama-stack-client==0.2.15
----
+
WARNING: If you are doing this with an older version of python (3.11 or less) you may not be able to install the matching version. Run using this instead **pip install llama-stack-client fire**

. Login to OpenShift if you are not already logged in
+
[source,bash,options="wrap",role="execute"]
----
oc login --server=https://api.${CLUSTER_NAME}.${BASE_DOMAIN}:6443 -u admin -p ${ADMIN_PASSWORD}
----

. Port forward the LlamaStack port so we can connect to it (in a workbench you can use the Service as the **--endpoint** argument or just **oc login** and then port-forward)
+
[source,bash,options="wrap",role="execute"]
----
oc -n <username>-llama-stack port-forward svc/llamastack-with-config-service 8321:8321 2>&1>/dev/null &
----
+
[IMPORTANT]
====
You will need to restart this port-forward every time the LlamaStack pod restarts.

Each new change to the LlamaStack ConfigMap (overlay path in the policy generator) causes the LlamaStack pod to restart. So keep the port-forward command handy in your history as you will need it!.
====

. Check the connection by listing the version - ideally we match client and server versions
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client inspect version
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/version "HTTP/1.1 200 OK"
VersionInfo(version='0.2.15')
----

. If you need help with the client commands, take a look at
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client --help
----

. Now list the providers - this should match what we have configured so far i.e. Tavily Web Search, Inference and agents
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client providers list
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/providers "HTTP/1.1 200 OK"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ API          â”ƒ Provider ID    â”ƒ Provider Type          â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ inference    â”‚ vllm           â”‚ remote::vllm           â”‚
â”‚ tool_runtime â”‚ tavily-search  â”‚ remote::tavily-search  â”‚
â”‚ agents       â”‚ meta-reference â”‚ inline::meta-reference â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

----

. Check the LlamaStack OpenAPI docs at http://localhost:8321/docs
+
image::llama-stack-api-docs.png[LlamaStack API Docs, 800]
+
TIP: Browsing will not work in a workbench

. Now list the models
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client models list
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models "HTTP/1.1 200 OK"

Available Models

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ model_type                   â”ƒ identifier                                               â”ƒ provider_resource_id                                     â”ƒ metadata               â”ƒ provider_id                   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ llm                          â”‚ granite-31-2b-instruct                                   â”‚ granite-31-2b-instruct                                   â”‚                        â”‚ vllm                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total models: 1

----
. Done âœ…

=== LlamaStack User Interface

. LlamaStack comes with a simple UI. [TODO: Add info about Playground]

+
image::llama-stack-playground2.png[LlamaStack Playground UI, 800]

. We can Chat with the LLM - this calls the **/v1/chat/completion** endpoint that we can find in the OpenAI docs for the vLLM served model. You can prompt it to check the connection is working. Ask the LLM a more complex question such as:
+
[source,bash,options="wrap",role="execute"]
----
What is LlamaStack ?
----
to see if it contains that information.
+
image::llama-stack-playground-hello.png[LlamaStack Playground Hello, 800]

+
[source,bash,options="wrap",role="execute"]
----
What is the weather today in Brisbane ?
----
+
In the playground this actually uses the **Regular** agent to call the Tool. The LLM makes its own decision to call the Tool. The tool returns a result to LLM and allows LLM to perform a new decision. This process loops until the LLM decides that a result can be provided to the user or certain conditions are met. The LLM produces a final result for the agent.
+
image::agent.png[Agent, 600]
+
image::llama-stack-websearch-agent.png[LlamaStack Websearch Agent, 800]

. Try out the **ReAct** agent to call the tool with the same prompt:
+
[source,bash,options="wrap",role="execute"]
----
What is the weather today in Brisbane ?
----
+
Notice that the agent first Reasons - where the LLM thinks about the data or tool results, Acts - where the LLM performs an action, LLM then Observes the result of the tool call, before returning the result. This is the essence of the ReAct agent pattern.
+
image::react-agent.png[ReAct Agent, 600]
+
image::llama-stack-websearch-react.png[LlamaStack Websearch ReAct Agent, 800]

. Done âœ…


=== LlamaStack mcp::openshift

MCP is an upcoming, popular standard for tool discovery and execution. It is a protocol that allows tools to be dynamically discovered from an MCP endpoint and can be used to extend the agentâ€™s capabilities.

First we need to deploy the pod that runs the https://github.com/containers/kubernetes-mcp-server[**mcp::openshift**,window=_blank] functions. Then we need to configure LlamaStack to use our first MCP tool that interacts with the OpenShift cluster. MCP servers are configured similarly to the tool and toolgroup provider.

. Rename the file https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/app-of-apps/sno/mcp-openshift.yaml.undeploy[**infra/app-of-apps/sno/mcp-openshift.yaml.undeploy**,window=_blank] -> https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/app-of-apps/sno/mcp-openshift.yaml[**infra/app-of-apps/sno/mcp-openshift.yaml**,window=_blank]
+
image::mcp-openshift-deploy.png[LlamaStack MCP OpenShift, 300]

. Check this file into git
+
[source,bash,options="wrap",role="execute"]
----
# Its not real unless its in git
git add .; git commit -m "deploy mcp::openshift"; git push
----

. Check Argo CD and ACM and for the MCP Pod
+
image::mcp-openshift-argocd.png[LlamaStack Basic Argo CD, 800]
+
image::mcp-openshift-acm.png[LlamaStack Basic ACM, 800]

. Check MCP Pod is running OK, check its logs
+
image::mcp-openshift-pod.png[LlamaStack Basic Pod, 800]

. Next we need to configure LlamaStack. Open the https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/applications/llama-stack/overlay/mcp-openshift/configmap.yaml[**mcp-openshift/configmap.yaml**,window=_blank] overlay and check where we add the tool runtime for MCP
+
[source,yaml,options="wrap"]
----
      tool_runtime:
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
----

. We also add in the tool group
+
[source,yaml,options="wrap"]
----
    tool_groups:
    - toolgroup_id: mcp::openshift
      provider_id: model-context-protocol
----

. Edit the file https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/applications/llama-stack/overlay/policy-generator-config.yaml[**infra/applications/llama-stack/overlay/policy-generator-config.yaml**,window=_blank] to point to the **mcp-openshift/** folder
+
image::llama-stack-mcp-openshift.png[LlamaStack MCP OpenShift, 300]

. Check these files into git
+
[source,bash,options="wrap",role="execute"]
----
# Its not real unless its in git
git add .; git commit -m "deploy llama-stack with mcp-openshift"; git push
----

. Refresh the playground in the browser. Select the **Tools** playground with the **MCP Servers openshift**, **ReAct agent** and **Llama4 model**. Try the prompt:
+
[source,bash,options="wrap",role="execute"]
----
list pods using the label app=ocp-mcp-server in the agent-demo namespace
----
+
image::llama-playground-mcp-openshift-chat.png[LlamaStack MCP OpenShift, 800]

. You will notice the the response has the Pod yaml included OK but fails to parse correctly in the llamastack-playground UI.

. The pydantic errors can be seen in the playground pod logs.
+
[source,bash,options="wrap",role="execute"]
----
oc -n llama-stack -c llama-stack-playground logs -l app.kubernetes.io/instance=llama-stack-playground
----

. Try this prompt instead using the same settings **MCP Servers openshift**, **ReAct agent** and **Llama4 model**
+
[source,bash,options="wrap",role="execute"]
----
list pods using the label app=ocp-mcp-server in the agent-demo namespace. dont give me the pod yaml, rather just give me the pod name
----
+
image::llamastack-playground-pod-search-no-error.png[LlamaStack MCP OpenShift No Error, 400]
+
This works without any parsing errors in the playground OK

. Try different models, agents and prompts. Not all of them work all of the time. This is a common problem with Tool calling and LLMs.

. Done âœ…

=== LlamaStack mcp::github

The last MCP Server we need to deploy https://github.com/github/github-mcp-server[interacts with GitHub,window=_blank]. The configuration is very similar to MCP OpenShift.

. Rename the file https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/app-of-apps/sno/mcp-github.yaml.undeploy[**infra/app-of-apps/sno/mcp-github.yaml.undeploy**,window=_blank] -> https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/app-of-apps/sno/mcp-github.yaml[**infra/app-of-apps/sno/mcp-github.yaml**,window=_blank]
+
image::mcp-github-deploy.png[LlamaStack MCP GitHub, 300]

. Check this file into git
+
[source,bash,options="wrap",role="execute"]
----
# Its not real unless its in git
git add .; git commit -m "deploy mcp::github"; git push
----

. Check Argo CD and ACM and for the MCP Pod
+
image::mcp-github-argocd.png[LlamaStack MCP GitHub Argo CD, 800]
+
image::mcp-github-acm.png[LlamaStack MCP GitHub ACM, 800]

. Check MCP Pod is running OK, check its logs
+
image::mcp-github-pod.png[LlamaStack MCP GitHub Pod, 800]

. We add in the tool group. Notice that the URI for the MCP Server uses Server Sent Events (/sse).
+
[source,yaml,options="wrap"]
----
    tool_groups:
    - toolgroup_id: mcp::github
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://github-mcp-server.agent-demo.svc.cluster.local:80/sse
----

. Edit the file https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/applications/llama-stack/overlay/policy-generator-config.yaml[**infra/applications/llama-stack/overlay/policy-generator-config.yaml**,window=_blank] to point to the **mcp-github/** folder
+
image::llama-stack-mcp-github.png[LlamaStack MCP GitHub, 300]

. Check these files into git
+
[source,bash,options="wrap",role="execute"]
----
# Its not real unless its in git
git add .; git commit -m "deploy llama-stack with mcp-github"; git push
----

. Try this prompt instead using the same settings **MCP Servers openshift**, **ReAct agent** and **Llama4 model**

. Refresh the playground in the browser. Select the **Tools** playground with the **MCP Servers github**, **ReAct agent** and **Llama4 model**. Try the prompt (replace the github user with your user).
+
[source,bash,options="wrap",role="execute"]
----
List the branches from ${YOUR_GITHUB_USER}/etx-agentic-ai repo.
----
+
image::llama-playground-mcp-github-chat.png[LlamaStack MCP GitHub, 800]
+
You can see also the response in the pod logs for the mcp::github server in the agent-demo namespace if you wish to debug any further.

. Try different models, agents and prompts. Not all of them work all of the time. This is a common problem with Tool calling and LLMs

. Done âœ…


=== LlamaStack Observability

LlamaStack integrates with the Observability stack we deployed as part of the bootstrap. The observability stack has a lot of moving parts. Traces are sent from LlamaStack via OTEL to a Tempo sink endpoint. We can then view traces in OpenShift using the Observe > Traces dashboard.

. To configure telemetry on our LlamaStack server, edit the file https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/applications/llama-stack/overlay/policy-generator-config.yaml[**infra/applications/llama-stack/overlay/policy-generator-config.yaml**,window=_blank] to point to the **sno/** folder. This is the final configuration for our use case.
+
image::llama-stack-sno.png[LlamaStack Telemetry, 300]

. Checking the base ConfigMap shows the telemetry stanza with the service name, sinks and the OTEL Tracing endpoint which is set as an environment variable on the Deployment
+
[source,yaml,options="wrap"]
----
      telemetry:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          service_name: ${env.OTEL_SERVICE_NAME:=llama-stack}
          sinks: ${env.TELEMETRY_SINKS:=console, sqlite, otel_metric, otel_trace}
          otel_exporter_otlp_endpoint: ${env.OTEL_EXPORTER_OTLP_ENDPOINT:=}
          sqlite_db_path: ${env.SQLITE_DB_PATH:=~/.llama/distributions/remote-vllm/trace_store.db}
----

. Check these files into git
+
[source,bash,options="wrap",role="execute"]
----
# Its not real unless its in git
git add .; git commit -m "deploy llama-stack with telemetry"; git push
----

. Refresh the playground in the browser. Select the Tools playground and select websearch, the MCP github Server and MCP OpenShift tool, ReAct agent and Llama4 model. Try out some of the previous prompting that include Tool calls to generate some traces.
+
image::llama-stack-traces1.png[LlamaStack Traces, 800]
+
image::llama-stack-traces2.png[LlamaStack Traces, 800]

. Done âœ…

=== LlamaStack Configured

. Check the completed LlamaStack configuration
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client providers list
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/providers "HTTP/1.1 200 OK"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ API          â”ƒ Provider ID            â”ƒ Provider Type                  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ scoring      â”‚ basic                  â”‚ inline::basic                  â”‚
â”‚ scoring      â”‚ llm-as-judge           â”‚ inline::llm-as-judge           â”‚
â”‚ agents       â”‚ meta-reference         â”‚ inline::meta-reference         â”‚
â”‚ inference    â”‚ vllm                   â”‚ remote::vllm                   â”‚
â”‚ inference    â”‚ vllm-llama-3-2-3b      â”‚ remote::vllm                   â”‚
â”‚ inference    â”‚ vllm-llama-4-guard     â”‚ remote::vllm                   â”‚
â”‚ inference    â”‚ sentence-transformers  â”‚ inline::sentence-transformers  â”‚
â”‚ tool_runtime â”‚ model-context-protocol â”‚ remote::model-context-protocol â”‚
â”‚ tool_runtime â”‚ brave-search           â”‚ remote::brave-search           â”‚
â”‚ tool_runtime â”‚ tavily-search          â”‚ remote::tavily-search          â”‚
â”‚ telemetry    â”‚ meta-reference         â”‚ inline::meta-reference         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
----
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client models list
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models "HTTP/1.1 200 OK"

Available Models

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ model_type      â”ƒ identifier                                                         â”ƒ provider_resource_id                    â”ƒ metadata                                    â”ƒ provider_id                  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ llm             â”‚ vllm/granite-31-2b-instruct                                        â”‚ granite-31-2b-instruct                  â”‚                                             â”‚ vllm                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ llm             â”‚ vllm-llama-3-2-3b/llama-3-2-3b                                     â”‚ llama-3-2-3b                            â”‚                                             â”‚ vllm-llama-3-2-3b            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ llm             â”‚ vllm-llama-4-guard/llama-4-scout-17b-16e-w4a16                     â”‚ llama-4-scout-17b-16e-w4a16             â”‚                                             â”‚ vllm-llama-4-guard           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ embedding       â”‚ sentence-transformers/all-MiniLM-L6-v2                             â”‚ all-MiniLM-L6-v2                        â”‚ {'embedding_dimension': 384.0}              â”‚ sentence-transformers        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total models: 4
----

. Done âœ…

=== Using the DataScienceCluster Resource to configure the LlamaStack Operator

This section is for information only and should be supported in 2.23+ of RHOAI.

[NOTE]
====
**VERSION 2.22 DOES NOT HAVE USERCONFIG MAP OVERRIDE SO DO NOT USE DSC YET - SCHEDULED FOR 2.23 **

With the latest version of RHOAI 2.22.0+ we can use the built in DSC (Data Science Cluster) mechanism to deploy the operator.

. Ensure the DataScienceCluster resource has the **llamastackoperator** component as **Managed**

. First, navigate to the correct project via
+
[source,bash,options="wrap",role="execute"]
----
oc project agent-demo
----

. Check the DataScienceCluster resource
+
[source,bash,options="wrap",role="execute"]
----
oc get dsc -o yaml
----

. Finally, review the DataScienceCluster resource to ensure the **llamastackoperator** component is set to **Managed**
+
[source,yaml,options="wrap"]
----
apiVersion: datasciencecluster.opendatahub.io/v1
kind: DataScienceCluster
metadata:
  name: default-dsc
spec:
  components:
    ...
    llamastackoperator:
      managementState: Managed
    ...
status: {}
----

. Check the LlamaStack controller manager pod is running in the **redhat-ods-applications** Namespace
+
image::llama-stack-controller-manager.png[LlamaStack Controller Manager Pod, 800]
====

=== (Optional) Bonus Extension Exercise

â›·ï¸ For the adventurous who like to go off-piste. Try to configure the RAG tool in LlamaStack and LlamaStack Playground using https://milvus.io/[milvus,window=_blank] as the vector store provider. â›·ï¸

[TIP]
====
We have left some code that is commented out and undeployed to help you get to the bottom of the ski run. Good Luck ğŸ«¡

- https://github.com/redhat-ai-services/etx-agentic-ai/tree/main/infra/applications/milvus[milvus,window=_blank] standalone application
- https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/app-of-apps/sno/milvus.yaml.undeploy[milvus app-of-apps,window=_blank] undeployed
- https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/applications/llama-stack/base/configmap.yaml#L105-L113[vector-io,window=_blank] LlamaStack configuration
- https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/applications/llama-stack/base/configmap.yaml#L95-L96[rag provider,window=_blank] LlamaStack configuration
- https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/applications/llama-stack/base/configmap.yaml#L125-L126[built-in rag toolgroup,window=_blank] LlamaStack configuration
- https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/working_with_rag/working-with-llama-stack_rag[Red Hat LlamaStack product documentation,window=_blank]
- https://github.com/opendatahub-io/llama-stack-demos[RedHat LlamaStack demo,window=_blank] repository from product documentation
====

== Using LlamaStack Playground for Use Case prompting

Use the Playground for our actual Use Case prompting (requires pipeline failure pods). We can try out various prompts to see what works best. Here are some examples - you will need to change the pod name.

=== Search for pod logs

- Model: **llama-4-scout-17b-16e-w4a16**
- Agent Type: **ReAct**
- Tools: **mcp::openshift**
+
Query the namespace pod for a pod error; copy-paste search for solution to error logs
+
Try this prompt (replace the pod name for the **java-app-build-run-bad-** in your **demo-pipeline** namespace)
+
[source,bash,options="wrap",role="execute"]
----
Review the OpenShift logs for the pod 'java-app-build-run-bad-7rdzn-build-pod', in the 'demo-pipeline' namespace.
----
+
image::playground-prompt-1.png[LlamaStack Traces, 800]
+
The agent reasoned correctly about the need to format the tool call with a container name as well
+
Try this prompt (replace the pod name for the **java-app-build-run-bad-** in your **demo-pipeline** namespace)
+
[source,bash,options="wrap",role="execute"]
----
Review the OpenShift logs for the container 'step-s2i-build' in pod 'java-app-build-run-bad-7rdzn-build-pod', in the 'demo-pipeline' namespace.
----
+
image::playground-prompt-2.png[LlamaStack Traces, 800]
+
Supplying the container name means less agent turns, less error prone

=== Search for pod logs and websearch error summary

- Model: **llama-4-scout-17b-16e-w4a16**
- Agent Type: **ReAct**
- Tools: **mcp::openshift, websearch**
+
Try this prompt (replace the pod name for the **java-app-build-run-bad-** in your **demo-pipeline** namespace)
+
[source,bash,options="wrap",role="execute"]
----
Review the OpenShift logs for the container 'step-s2i-build' in pod 'java-app-build-run-bad-7rdzn-build-pod', in the 'demo-pipeline' namespace. If the logs indicate an error search for the top OpenShift solution. Create a summary message with the category and explanation of the error.
----
+
image::playground-prompt-3.png[LlamaStack Traces, 800]
+
image::playground-prompt-4.png[LlamaStack Traces, 800]
+
The Agent retrieved the pod logs and performed a websearch
+
Compare the results to the actual pod logs.
+
[source,bash,options="wrap",role="execute"]
----
oc -n demo-pipeline -c step-s2i-build logs java-app-build-run-bad-7rdzn-build-pod
----

=== Create a GitHub issue

- Model: **llama-4-scout-17b-16e-w4a16**
- Agent Type: **ReAct**
- Tools: **mcp::github**
+
Create a GitHub issue, add add issue comments
+
Try this prompt (replace the github user with your user).
+
[source,bash,options="wrap",role="execute"]
----
Create a github issue for a fake error in the ${YOUR_GITHUB_USER}/etx-agentic-ai repo and assign it to ${YOUR_GITHUB_USER}.
----
+
image::playground-prompt-5.png[LlamaStack Traces, 800]
+
This fails due to parsing json arrays as strings. Try this prompt (replace **YOUR_GITHUB_USER** with your user).
+
[source,bash,options="wrap",role="execute"]
----
Create a github issue using these parameters {"name":"create_issue","arguments":{"owner":"${YOUR_GITHUB_USER}","repo":"etx-agentic-ai","title":"Fake Error: Agentic AI Service Unresponsive","body":"The Agentic AI service is not responding. This is a fake error report."}}} DO NOT add any optional parameters.
----
+
image::playground-prompt-6.png[LlamaStack Traces, 800]
+
This is successful but may take a few turns to get the tool call right
+
image::playground-prompt-8.png[LlamaStack Traces, 800]

=== Create the Final Prompt

- Model: **llama-4-scout-17b-16e-w4a16**
- Agent Type: **ReAct**
- Tools: **mcp::openshift, mcp::github**
+
Prompt engineering; let's try and create a prompt that chains together the different tasks together, analyze the pod logs then generate a github issue ğŸº Try this prompt (replace **YOUR_GITHUB_USER** with your user, replace the pod name for the **java-app-build-run-bad-** in your **demo-pipeline** namespace)
+
[source,bash,options="wrap",role="execute"]
----
You are an expert OpenShift administrator. Your task is to analyze pod logs, summarize the error, and generate a JSON object to create a GitHub issue for tracking. Follow the format in the examples below.

---
EXAMPLE 1:
Input: The logs for pod 'frontend-v2-abcde' in namespace 'webapp' show: ImagePullBackOff: Back-off pulling image 'my-registry/frontend:latest'.

Output:
The pod is in an **ImagePullBackOff** state. This means Kubernetes could not pull the container image 'my-registry/frontend:latest', likely due to an incorrect image tag or authentication issues.
{"name":"create_issue","arguments":{"owner":"${YOUR_GITHUB_USER}","repo":"etx-agentic-ai","title":"Issue with Etx pipeline","body":"### Cluster/namespace location\\nwebapp/frontend-v2-abcde\\n\\n### Summary of the problem\\nThe pod is failing to start due to an ImagePullBackOff error.\\n\\n### Detailed error/code\\nImagePullBackOff: Back-off pulling image 'my-registry/frontend:latest'\\n\\n### Possible solutions\\n1. Verify the image tag 'latest' exists in the 'my-registry/frontend' repository.\\n2. Check for authentication errors with the image registry."}}

---
EXAMPLE 2:
Input: The logs for pod 'data-processor-xyz' in namespace 'pipelines' show: CrashLoopBackOff. Last state: OOMKilled.

Output:
The pod is in a **CrashLoopBackOff** state because it was **OOMKilled**. The container tried to use more memory than its configured limit.
{"name":"create_issue","arguments":{"owner":"${YOUR_GITHUB_USER}","repo":"etx-agentic-ai","title":"Issue with Etx pipeline","body":"### Cluster/namespace location\\npipelines/data-processor-xyz\\n\\n### Summary of the problem\\nThe pod is in a CrashLoopBackOff state because it was OOMKilled (Out of Memory).\\n\\n### Detailed error/code\\nCrashLoopBackOff, Last state: OOMKilled\\n\\n### Possible solutions\\n1. Increase the memory limit in the pod's deployment configuration.\\n2. Analyze the application for memory leaks."}}
---

NOW, YOUR TURN:

Input: Review the OpenShift logs for the container 'step-s2i-build' for the pod 'java-app-build-run-bad-7rdzn-build-pod' in the 'demo-pipeline' namespace. If the logs indicate an error, search for the solution, create a summary message with the category and explanation of the error, and create a Github issue using {"name":"create_issue","arguments":{"owner":"${YOUR_GITHUB_USER}","repo":"etx-agentic-ai","title":"Issue with Etx pipeline","body":"<summary of the error>"}}. DO NOT add any optional parameters.

ONLY tail the last 10 lines of the pod, no more.
The JSON object formatted EXACTLY as outlined above.
----
+
The final prompt linking pod log errors and github
+
video::final-prompt.webm[type=video/webm, 600]
+
image::playground-prompt-9.png[LlamaStack Traces, 800]
+
And the GitHub issue successfully created ğŸ†

. Done âœ…



// lightbox - for images - FIXME need to make the include::partial$lightbox.hbs WORK
++++
<div id="myModal" class="modal">
    <span class="close cursor" onclick="closeModal()">&times;</span>
    <div class="modal-content" onclick="closeModal()">
        <!--suppress HtmlRequiredAltAttribute as this will be set when selecting the image via JavaScript,
        RequiredAttributes as src will be set by when selecting the image via JavaScript -->
        <img id="imageinmodal">
    </div>
</div>
<script>
    function openModal() {
        document.getElementById("myModal").style.display = "block";
        // use overflowY = hidden to prevent the body from scrolling when modal is visible
        // doesn't work with overscroll-behavior, as this would work only when the modal has a scrollbar
        document.getElementsByTagName("body")[0].style.overflowY = "hidden";
    }

    function closeModal() {
        document.getElementById("myModal").style.display = "none";
        document.getElementsByTagName("body")[0].style.overflowY = "auto";
    }

    document.querySelectorAll('.imageblock img').forEach(element => {
        if (element.closest('a') === null) {
            element.className += " lightbox";
            element.addEventListener('click', evt => {
                document.getElementById("imageinmodal").setAttribute("src", evt.currentTarget.getAttribute("src"))
                document.getElementById("imageinmodal").setAttribute("alt", evt.currentTarget.getAttribute("alt"))
                openModal();
            })
        }
    });
</script>
<style>
    /* The Modal (background) */
    .modal {
        display: none;
        position: fixed;
        z-index: 10;
        padding-top: 5vh;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        overflow: auto;
        backdrop-filter: blur(3px);
        background-color: rgba(30, 30, 30, 0.8);
    }
    img.lightbox {
        cursor: pointer;
    }
    /* Modal Content */
    .modal-content {
        position: relative;
        margin: auto;
        padding: 0;
        width: 90%;
        max-height: 90vh;
        cursor: pointer;
    }

    .modal-content img {
        width: auto;
        height: auto;
        max-width: 90vw;
        max-height: 90vh;
        min-width: 90vw;
        min-height: 90vh;
        display: block;
        margin-right: auto;
        margin-left: auto;
        object-fit: contain;
    }

    /* The Close Button */
    .close {
        color: white;
        position: absolute;
        top: 10px;
        right: 25px;
        font-size: 35px;
        font-weight: bold;
    }

    .close:hover,
    .close:focus {
        color: #999;
        text-decoration: none;
        cursor: pointer;
    }
</style>
++++

== Artifacts to carry forward

* Selected model and endpoint details (Playground URL, model id)
* Finalized system prompt and example user prompts used in Playground
* Tooling decisions (builtin websearch enabled, MCP servers registered)
* Any safety/guardrail parameters you validated
* Notes/screenshots of successful Playground runs
