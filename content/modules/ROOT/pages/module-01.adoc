= Lab Setup and Prerequisites

include::vars.adoc[]

[NOTE]
====
Persona: Platform Engineer (primary). Also relevant: AI Engineer.
====

[NOTE]
====
Estimated time: 30 minutes
====

== What you'll do

* Verify cluster access and bootstrap GitOps
* Fork the repo, set PATs and webhooks
* Ensure Console Links and core apps are healthy

== Prerequisites

Before starting this lab, ensure you have the following:

=== Required Tools

* **GitHub Account**: You'll need access to fork repositories and collaborate
* **Git CLI**: Installed and configured on your local machine
* **OpenShift CLI (oc)**: Download from https://mirror.openshift.com/pub/openshift-v4/clients/ocp/[OpenShift client downloads,window=_blank]
* **jq**: Command-line JSON processor for parsing command outputs
* **Python 3**: Provides `json.tool` for pretty-printing JSON
* **envsubst (gettext)**: For environment variable substitution in manifests
* **Terminal/Command Line**: Bash shell with basic utilities (openssl, curl)
* **Ansible Vault**: Part of Ansible toolkit for secret management
* **Web Browser**: For accessing OpenShift console, MaaS portal, and documentation

=== Required Skills
* **Basic Git Knowledge**: Cloning repositories, basic version control concepts
* **Command Line Basics**: Navigating directories, running commands
* **Container Concepts**: Understanding of containers and Kubernetes/OpenShift (helpful but not required)

=== Provided by Instructors
* **OpenShift Cluster Access**: Admin credentials
* **Red Hat MaaS Access**: Model-as-a-Service credentials for LLaMA models
* **Workshop Materials**: All necessary configuration files and scripts
* **Environment Variables**: Cluster-specific configuration values
* **Support**: Technical assistance throughout the lab

== Getting Started

Before diving into the agentic AI lab, we need to set up our development environment. This involves:

* **Setting up GitOps**: Configure automated deployment pipelines that manage our infrastructure and applications

[IMPORTANT]
====
**For this lab**: We have used automated https://github.com/rhpds/etx-llmaas-gitops[bootstrap scripts,window=_blank] to handle the setup quickly so we can focus on building AI agents.
====

**For later exploration** (after the lab):

* **Manual setup**: The step-by-step instructions below show how each component works - perfect for understanding GitOps and secret management in detail

== Initial Setup

. **Receive your cluster credentials** ðŸ”
+
Your instructor will provide OpenShift login credentials for your cluster.

. **Set up your shared repository**:
.. https://github.com/redhat-ai-services/etx-agentic-ai[Fork the etx-agentic-ai repository,window=_blank] to your personal GitHub account
+
.GitHub Repo Fork
image::github-fork.png[GitHub Repo Fork, 400]
+

.. Ensure that you **Enable Issues** for your fork under **Settings** > **General** > **Features** > **Issues** as they are disabled for forked repos by default
+
.GitHub Repo Enable Issues
image::github-repo-enable-issues.png[GitHub Repo Enable Issues, 400]

. **Setup and run the Web Terminal Operator as described in the Quick Starts**

. **Later in the lab we will clone the forked repository to the OpenShift Web Terminal through steps in the Quick Starts, but if you're running the lab locally you can go ahead and clone the repository locally to your laptop.**
+
[source,bash,options="wrap",role="execute"]
----
git clone git@github.com:your-gh-user/etx-agentic-ai.git
cd etx-agentic-ai
----
+
.GitHub Repo Clone
image::github-clone.png[GitHub Repo Clone, 400]
+
[TIP]
====
Replace `your-gh-user` with the actual GitHub username of whoever forked the repository.
====

. **Verify your setup** âœ…
+
You should now have:
+
* Access to your OpenShift cluster
* A fork of the repository
* Local copies of the code

== Cluster Environment

You have now access to a fully-featured OpenShift cluster designed for AI workloads. This cluster mimics many customer production environments. Here's how the platform is architected:

=== Bootstrap Components
These foundational components are deployed first to establish the platform's operational baseline:

* **Red Hat OpenShift**: Enterprise Kubernetes platform providing container orchestration
* **Argo CD**: Declarative, Git-driven application deployments
* **OpenShift Secrets**: Secure credential storage and automated secret injection

=== Security, Governance, and Delivery (Argo CD + Policy as Code)
Built on the bootstrap foundation, we combine continuous delivery with continuous compliance:

[IMPORTANT]
.Why both exist in this repo
====
* **Argo CD (delivery)** deploys the capabilities that teams need (Llama Stack, MCP tools, agent services, pipelines) from Git
* **Policy as Code (governance)** enforces the rules those capabilities must follow (security, sources, sizes, hygiene)

What policies can assert here:

* **Model source allowlists**: only approved providers/endpoints (legal/commercial constraints)
* **Model size limits**: keep parameter/VRAM footprint within cost/perf targets
* **Image/registry restrictions**: approved registries and signed images
* **Resource hygiene**: every Pod declares requests/limits; GPU workloads use time-slicing/quotas
* **Secret handling**: Vault or external secret injection; no plaintext k8s secrets
* **Network posture**: NetworkPolicies limiting egress to approved services (e.g., Llama Stack, GitHub MCP)

This pairing yields:

* **Zero Configuration Drift**: what's in Git is what runs (Argo CD)
* **Automated Compliance**: guardrails are applied continuously (policies)
* **Green from GO**: start compliant; learn inside enterprise guardrails from day one

NOTE: Policy as Code is not strictly required to complete this lab. It is, however, a highly useful paradigmâ€”and in some environments or delivery workflows it is requiredâ€”so we model it here to reflect real-world practices.

Learn more:

* https://open-cluster-management.io/docs/getting-started/integration/policy-controllers/policy/[Policy API Concepts,window=_blank]
* https://github.com/open-cluster-management-io/policy-collection[Policy Collection,window=_blank]
* https://github.com/open-cluster-management-io/policy-generator-plugin[Policy Generator,window=_blank]
====

.Policy as Code using GitOps and ACM
image::policy-as-code.png[Policy as Code, 600]

=== Developer Platform Services
Self-service capabilities that enable development teams:

* **CI/CD Pipelines**: Tekton for automated container builds, testing, and deployments
* **Source Control Integration**: Git-based workflows with automated quality gates
* **Container Registry**: Secure image storage with vulnerability scanning and promotion workflows

=== Tenant & Workload Services
Multi-tenant capabilities providing isolated, secure environments:

* **Namespace Management**: Multi-tenant isolation with RBAC and resource quotas
* **Development Workbenches**: Self-service Jupyter environments for data science teams
* **Service Mesh**: Secure service-to-service communication and traffic management

=== AI/ML Platform Services
Specialized services for AI/ML workloads and agentic applications:

* **Red Hat OpenShift AI (RHOAI)**: Managed AI/ML platform with GPU acceleration
* **Model Serving Infrastructure**: Scalable inference endpoints with model lifecycle management
* **Agentic AI Runtime**: Environment for deploying AI agents with external service integrations

[TIP]
====
**LLaMA Stack Integration**: Our agentic AI workloads leverage https://github.com/llamastack/llama-stack[LLaMA Stack,window=_blank], a composable framework that provides standardized APIs for model inference, safety guardrails, and tool integration. This allows our AI agents to seamlessly interact with large language models while maintaining consistent interfaces for memory management, tool calling, and safety controls across different model providers.
====

**The Benefits:**

* **ZERO configuration drift** - what's in git is real
* **Integrates into the Governance Dashboard in ACM for SRE**
* **We start as we mean to go on** - we are Green from GO so that our dev environment looks like prod only smaller
* **All our clusters and environments are Kubernetes Native once bootstrapped**

== Required Applications

You need to do each of these Prerequisites.

. We **Recommend** using the **Quick Starts** initially, then your laptop after the initial setup.
+
Start with the **Quick Start** - **ETX 1 The Basics**

. You may choose another client to bootstrap from (not recommended **especially** if you are on **MacOSX** which is not fully tested). It could be:
** Your https://www.dell.com/en-au/shop/dell-laptops/xps-16-laptop/spd/xps-16-9640-laptop[Laptop,window=_blank] or a https://docs.fedoraproject.org/en-US/fedora-silverblue/toolbox[Toolbx,window=_blank] or a https://docs.fedoraproject.org/en-US/docs/[Fedora like jumphost,window=_blank] or a https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/working_on_data_science_projects/using-project-workbenches_projects#creating-a-project-workbench_projects[Workbench Terminal,window=_blank] that can access your cluster and the internet
** Your bootstrap client must have a https://packages.fedoraproject.org/pkgs/bash/bash/[bash,window=_blank] shell with https://packages.fedoraproject.org/pkgs/openssl/openssl/[openssl,window=_blank], https://docs.ansible.com/ansible/2.9/cli/ansible-vault.html#ansible-vault[ansible-vault,window=_blank] installed
** Login to your OpenShift cluster using the https://mirror.openshift.com/pub/openshift-v4/clients/ocp/[OpenShift client,window=_blank] as the cluster-admin user

. Setup env vars and login to OpenShift. You can either use the OpenShift Web Console to grab a login command or use the details below to construct the login command on your own.
+
TIP: Covered in a Quick Start
+
[source,bash,options="wrap",role="execute"]
----
export ADMIN_PASSWORD=password # replace with yours
export CLUSTER_NAME=ocp.4ldrd # replace with yours
export BASE_DOMAIN=sandbox2518.opentlc.com # replace with yours
----
+
[source,bash,options="wrap",role="execute"]
----
oc login --server=https://api.${CLUSTER_NAME}.${BASE_DOMAIN}:6443 -u admin -p ${ADMIN_PASSWORD}
----

. Done âœ…

=== MaaS credentials

[NOTE]
====
This whole section can be skipped unless you wish to use your own Model as a Service Credentials. We will share the provided credentials to save time.

Gather your Model as a Service Credentials.

. Login to https://maas.apps.prod.rhoai.rh-aiservices-bu.com[Models-as-a-service using your RedHat credentials,window=_blank].
. Click on the __See your Applications & their credentials__ button.
. Create 3 Applications for these three models
** **Llama-3.2-3B**
** **Llama-4-Scout-17B-16E-W4A16**
** **Nomic-Embed-Text-v1.5**
+
e.g. for example __llama-4-scout-17b-16e-w4a16__
+
.MaaS LLama4 Scout
image::maas-llama-4-scout-17b-16e-w4a16.png[MaaS LLama4 Scout, 400]

. Setup env vars
+
[source,bash,options="wrap",role="execute"]
----
export MODEL_LLAMA3_API_KEY=e3...
export MODEL_LLAMA3_ENDPOINT_URL=https://llama-3-2-3b-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443
export MODEL_LLAMA3_NAME=llama-3-2-3b

export MODEL_LLAMA4_API_KEY=ce...
export MODEL_LLAMA4_ENDPOINT_URL=https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443
export MODEL_LLAMA4_NAME=llama-4-scout-17b-16e-w4a16

export MODEL_EMBED_API_KEY=95...
export MODEL_EMBED_URL=https://nomic-embed-text-v1-5-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443
export MODEL_EMBED_NAME=/mnt/models
----

. Done âœ…
====

=== Tavily search token

Gather your Tavily web search API Key.

. Setup a https://app.tavily.com[Tavily,window=_blank] api key for web search. Login using a github account of one of your team members.
+
.Tavily API Key
image::tavily-apikey.png[Create Tavily API Key, 600]

. Done âœ…

=== GitHub Token

Create a fine-grained GitHub Personal Access (PAT) Token.

. Login to GitHub in a browser, then click on your user icon > **Settings**

. Select **Developer Settings** > **Personal Access Tokens** > **Fine-grained personal access tokens**

. Select Button **Generate a new token** - give it a token name e.g. __etx-ai__

. Set **Repository access**
+
**All repositories**: allow access to your repositories including read-only public repos.

. Give it the following permissions:
+
**Commit statuses**: Read-Only
+
**Content**: Read-Only
+
**Issues**: Read and Write
+
**Metadata**: Read-Only (this gets added automatically)
+
**Pull requests**: Read-Only
+
.GitHub Repo Perms
image::github-repo-perms.png[GitHub Repo Perms, 400]

. Generate the token.
+
.GitHub Repo Token
image::github-pat.png[GitHub Repo Token, 400]

. Done âœ…

=== GitHub Webhook

Create a webhook that fires from your GitHub repo fork to Argo CD on the OpenShift Cluster. This ensures the applications are synced whenever you push a change into git (rather than wait the 3min default sync time).

. Login to GitHub in a browser, go to your **etx-agentic-ai** fork > **Settings**

. Select **Webhooks**

. Select **Add Webhook**. Add the following details
+
**Payload URL:** https://global-policy-server-openshift-policy.CLUSTER_DOMAIN/api/webhook - You can get the correct URL by echoing this out on the command line:
+
[source,bash,options="wrap",role="execute"]
----
echo https://global-policy-server-openshift-policy.CLUSTER_DOMAIN/api/webhook
----
+
**Content Type:** application/json
+
**SSL Verification:** Enable SSL Verification
+
**Which events:** Send me everything

. Click **Add Webhook**
+
.GitHub Webhook
image::github-webhook.png[GitHub Webhook, 400]

. Done âœ…

== Complete the Bootstrap

. The following https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/console_apis/consolelink-console-openshift-io-v1[OpenShift ConsoleLinks,window=_blank] should already exist in your cluster:
+
image::add-console-links.png[Console Links, 300]
+
**Red Hat Applications** - these are cloud services provided by Red Hat for your cluster.
+
**GenAI** - these are the GenAI applications that we will be using in the exercises. The **Agentic Argo CD** should be running but is empty (no apps deployed yet) and is our GitOps application. The **LLamaStack Playground** is not deployed yet, but will be the link for the LlamaStack UI for integrating Tools and Agents. **Vault** is running and initialized and unsealed and is the app that stores our secrets.
+
**OpenShift GitOps** - this is the cluster bootstrap Argo CD GitOps. This has all of the setup to get started for our cluster. It does not include the Agentic applications that we cover in the exercises.
+
**RHOAI** - the UI for Red Hat OpenShift AI. Login here to access your Data Science workbenches, models, pipelines and experiments.

. Bootstrap https://argo-cd.readthedocs.io/en/latest/operator-manual/cluster-bootstrapping/#app-of-apps-pattern[App-of-Apps,window=_blank]
+
[source,bash,options="wrap",role="execute"]
----
# We need to update our Argo CD Apps to point to your team fork
export YOUR_GITHUB_USER=your-gh-user  # the Team member who forked the GitHub Repo
cd etx-agentic-ai   # Navigate to root directory of code base if not already there
----

. Replace the `redhat-ai-services` throughout the file with your GitHub username.
+
[source,bash,options="wrap",role="execute"]
----
sed -i "s/redhat-ai-services/${YOUR_GITHUB_USER}/g" infra/app-of-apps/etx-app-of-apps.yaml
----

. Update the `redhat-ai-services` to your GitHub username in the `etx-app-of-apps.yaml` file.
+
[source,bash,options="wrap",role="execute"]
----
for x in $(ls infra/app-of-apps/sno); do
    sed -i "s/redhat-ai-services/${YOUR_GITHUB_USER}/g" infra/app-of-apps/sno/$x
done
----

. Now we can save, commit, and push the changes to your GitHub fork.
+
[source,bash,options="wrap",role="execute"]
----
# Its not real unless its in git
git add .; git commit -m "using my github fork"; git push
----

. Finally, we can bootstrap the apps into our cluster.
+
[source,bash,options="wrap",role="execute"]
----
# Bootstrap all our apps
oc apply -f infra/app-of-apps/etx-app-of-apps.yaml
----
+
This will install the tenant pipeline app and observability stack into our cluster. All the other GenAI apps are undeployed for now. You can check this in your _app-of-apps/cluster-name_ github fork folder.
+
image::bootstrap-initial.png[bootstrap-initial, 400]

. Check the Install progress of the app-of-apps in the **Agentic Argo CD**
+
image::bootstrap-begin.png[bootstrap-begin, 400]

. You will need to wait for the individual apps to be installed. This may take a few minutes. After a few minutes, you should see the following output to show that the apps have been installed.
+
image::bootstrap-complete.png[bootstrap-complete, 400]
+
Also, notice that the `tenant-ai-agent-local-cluster` app is constantly in a progressing state. This is something we will address later in this course.

. Done âœ…

== Technical Knowledge

Ideally https://www.redhat.com/tracks/_pfcdn/assets/10330/contents/344388/925d2cb5-39c2-49dc-9ed2-3f4aeeb52a85.pdf[your team is a cross-functional one (Optional: Read Chapter.1 - Introduction),window=_blank] with:

* Good understanding of OpenShift/Kubernetes concepts
* Basic familiarity with Python programming
* Good knowledge of https://www.redhat.com/en/topics/containers[containerization,window=_blank] concepts
* Basic understanding of CI/CD pipelines
* Good grasp of https://openpracticelibrary.com/practice/gitops[GitOps,window=_blank] and https://openpracticelibrary.com/practice/everything-as-code[Everything as Code practices,window=_blank]

xref:module-01.adoc[â˜• Buckle Up], Here we go ...

// lightbox - for images - FIXME need to make the include::partial$lightbox.hbs WORK
++++
<div id="myModal" class="modal">
    <span class="close cursor" onclick="closeModal()">&times;</span>
    <div class="modal-content" onclick="closeModal()">
        <!--suppress HtmlRequiredAltAttribute as this will be set when selecting the image via JavaScript,
        RequiredAttributes as src will be set by when selecting the image via JavaScript -->
        <img id="imageinmodal">
    </div>
</div>
<script>
    function openModal() {
        document.getElementById("myModal").style.display = "block";
        // use overflowY = hidden to prevent the body from scrolling when modal is visible
        // doesn't work with overscroll-behavior, as this would work only when the modal has a scrollbar
        document.getElementsByTagName("body")[0].style.overflowY = "hidden";
    }

    function closeModal() {
        document.getElementById("myModal").style.display = "none";
        document.getElementsByTagName("body")[0].style.overflowY = "auto";
    }

    document.querySelectorAll('.imageblock img').forEach(element => {
        if (element.closest('a') === null) {
            element.className += " lightbox";
            element.addEventListener('click', evt => {
                document.getElementById("imageinmodal").setAttribute("src", evt.currentTarget.getAttribute("src"))
                document.getElementById("imageinmodal").setAttribute("alt", evt.currentTarget.getAttribute("alt"))
                openModal();
            })
        }
    });
</script>
<style>
    /* The Modal (background) */
    .modal {
        display: none;
        position: fixed;
        z-index: 10;
        padding-top: 5vh;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        overflow: auto;
        backdrop-filter: blur(3px);
        background-color: rgba(30, 30, 30, 0.8);
    }
    img.lightbox {
        cursor: pointer;
    }
    /* Modal Content */
    .modal-content {
        position: relative;
        margin: auto;
        padding: 0;
        width: 90%;
        max-height: 90vh;
        cursor: pointer;
    }

    .modal-content img {
        width: auto;
        height: auto;
        max-width: 90vw;
        max-height: 90vh;
        min-width: 90vw;
        min-height: 90vh;
        display: block;
        margin-right: auto;
        margin-left: auto;
        object-fit: contain;
    }

    /* The Close Button */
    .close {
        color: white;
        position: absolute;
        top: 10px;
        right: 25px;
        font-size: 35px;
        font-weight: bold;
    }

    .close:hover,
    .close:focus {
        color: #999;
        text-decoration: none;
        cursor: pointer;
    }
</style>
++++

== The Use Case

[NOTE]
====
Estimated time: 10â€“15 minutes
====

Acme Inc. is facing issues within their CI/CD system, and they're asking you to build a Gen AI based solution for them.

== The Challenge

Acme Inc. has a critical CI/CD pipeline that frequently fails due to various issues such as:

* Build failures due to dependency conflicts
* Deployment errors caused by configuration mismatches
* Resource constraints leading to timeouts
* Integration test failures requiring manual investigation

The current process requires developers to:

* Manually investigate error logs
* Search through documentation for solutions
* Create GitHub issues for tracking problems
* Manually restart failed pipelines
* Coordinate with team members for resolution

This manual intervention is:

* Time-consuming and delays releases
* Error-prone due to human oversight
* Not scalable as the team grows
* Lacking consistent documentation of solutions

== How to think about the problem (DORA and MTTR)

* DORA metrics are industry-standard measures of software delivery performance used to identify high performing teams: Deployment Frequency, Lead Time for Changes, Change Failure Rate, and Mean Time to Recovery (MTTR). See https://dora.dev[https://dora.dev,window=_blank] for background.
* MTTR is the average time it takes to restore service after a failure. Lower MTTR means faster recovery and less downtime.
* In later modules you will codify the agent and wire it into the pipelineâ€™s finally step so failures automatically trigger the agentâ€”helping drive down MTTR.

== The Solution: AI-Powered CI/CD Agent

We will build an intelligent agent that can:

* Detect and analyze CI/CD failures
* Interact with OpenShift to query cluster resources and pod status
* Search for relevant solutions using web search and documentation
* Provide actionable recommendations to developers
* Create GitHub issue in the specified repository for tracking

// lightbox - for images - FIXME need to make the include::partial$lightbox.hbs WORK
++++
<div id="myModal" class="modal">
    <span class="close cursor" onclick="closeModal()">&times;</span>
    <div class="modal-content" onclick="closeModal()">
        <!--suppress HtmlRequiredAltAttribute as this will be set when selecting the image via JavaScript,
        RequiredAttributes as src will be set by when selecting the image via JavaScript -->
        <img id="imageinmodal">
    </div>
</div>
<script>
    function openModal() {
        document.getElementById("myModal").style.display = "block";
        // use overflowY = hidden to prevent the body from scrolling when modal is visible
        // doesn't work with overscroll-behavior, as this would work only when the modal has a scrollbar
        document.getElementsByTagName("body")[0].style.overflowY = "hidden";
    }

    function closeModal() {
        document.getElementById("myModal").style.display = "none";
        document.getElementsByTagName("body")[0].style.overflowY = "auto";
    }

    document.querySelectorAll('.imageblock img').forEach(element => {
        if (element.closest('a') === null) {
            element.className += " lightbox";
            element.addEventListener('click', evt => {
                document.getElementById("imageinmodal").setAttribute("src", evt.currentTarget.getAttribute("src"))
                document.getElementById("imageinmodal").setAttribute("alt", evt.currentTarget.getAttribute("alt"))
                openModal();
            })
        }
    });
</script>
<style>
    /* The Modal (background) */
    .modal {
        display: none;
        position: fixed;
        z-index: 10;
        padding-top: 5vh;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        overflow: auto;
        backdrop-filter: blur(3px);
        background-color: rgba(30, 30, 30, 0.8);
    }
    img.lightbox {
        cursor: pointer;
    }
    /* Modal Content */
    .modal-content {
        position: relative;
        margin: auto;
        padding: 0;
        width: 90%;
        max-height: 90vh;
        cursor: pointer;
    }

    .modal-content img {
        width: auto;
        height: auto;
        max-width: 90vw;
        max-height: 90vh;
        min-width: 90vw;
        min-height: 90vh;
        display: block;
        margin-right: auto;
        margin-left: auto;
        object-fit: contain;
    }

    /* The Close Button */
    .close {
        color: white;
        position: absolute;
        top: 10px;
        right: 25px;
        font-size: 35px;
        font-weight: bold;
    }

    .close:hover,
    .close:focus {
        color: #999;
        text-decoration: none;
        cursor: pointer;
    }
</style>
++++

== Initial Exploration

[NOTE]
====
Persona: Data Scientist (primary). Also relevant: AI Engineer.
====

[IMPORTANT]
.In this module
====
Get hands-on with the platform: create an OpenShift AI Workbench, connect to the repo, and open your first notebook so youâ€™re ready for later labs.
====

[NOTE]
====
Estimated time: 20â€“30 minutes
====

== What you'll do

* Use Console Links to navigate
* Create a GPU-enabled Workbench
* Clone the lab repo into your Workbench
* Open and run the getting started notebook as a data scientist

== Getting Started

=== Explore the environment

Use the OpenShift Console Links to quickly navigate around the environment.

* RHOAI
* LLamaStack Playground
* OpenShift
* Argo CD

image::console-links.png[Console Links, 300]

=== Pre-deployed Components

Let's look at some of the components that have already been deployed for this lab.

* LlamaStack Distribution
* Access to model through MaaS
* LlamaStack Playground
* Tavily Search Tool
* MCP: OpenShift Tool
* MCP: GitHub Tool

=== LlamaStack Distribution

We'll be creating a LlamaStack Distribution custom resource (CR) to define how a LlamaStack server should be deployed. It allows you to specify:

* **Server Configuration**: Which distribution to use (Ollama, vLLM, etc.)
* **Container Specifications**: Port, environment variables, resource limits

There is a unique instance of LlamaStack distribution deployed for each lab user.

// . Locate the LlamaStack distribution.
// +
// image::llamastackdistribution-search.png[LlamaStack distribution, 800]
// +

. Click on the existing distribution.
+
image::llamastackdistribution-current.png[LlamaStack distribution current, 800]
+

. Click on the YAML tab
+
image::llamastackdistribution-tab.png[LlamaStack distribution tab, 800]
+

. Check out the LlamaStack Distribution we're using for this lab.

image::llamastackdistribution-yaml.png[LlamaStack distribution YAML, 800]

You will observe that we've specify the secrets for the services that'll be interacting with LlamaStack server:

* [1] MaaS API
* [2] Tavily search API.

Also listed is the user configuration used with this distribution (ConfigMap: llama-stack-config).

=== LlamaStack ConfigMap

A ConfigMap can be used to store run.yaml configuration for each LlamaStack distribution. Updates to the ConfigMap will restart the Pod to load the new data.

Example to create a run.yaml ConfigMap

[source,yaml,options="wrap"]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: llama-stack-config
  namespace: {{ .Values.username }}-llama-stack
data:
  run.yaml: |
    # Llama Stack configuration
    version: '2'
    image_name: vllm
    apis:
    - inference
    - tool_runtime
    - agents
    - safety
    - vector_io
    models:
      - metadata: {}
        model_id: {{ .Values.defaultModel.name }}
        provider_id: vllm-{{ .Values.defaultModel.name }}
        provider_model_id: {{ .Values.defaultModel.name }}
        model_type: llm
    providers:
      inference:
      - provider_id: vllm-{{ .Values.defaultModel.name }}
        provider_type: "remote::vllm"
        config:
          url: {{ .Values.defaultModel.url }}
          context_length: 4096
          api_token: ${env.DEFAULT_MODEL_API_TOKEN}
          tls_verify: true
      tool_runtime:
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_API_KEY}
          max_results: 3
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/agents_store.db
          responses_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/responses_store.db
    tools:
      - name: builtin::websearch
        enabled: true
    tool_groups:
    - provider_id: tavily-search
      toolgroup_id: builtin::websearch
    server:
      port: 8321
----


. Login to OpenShift AI and select the `agent-demo` data science project.
+
image::create-workbench2.png[Create Workbench, 600]

. We are going to `Create a workbench` using the following parameters:








=== Create a workbench

. Login to OpenShift AI and select the `agent-demo` data science project.
+
image::create-workbench2.png[Create Workbench, 600]

. We are going to `Create a workbench` using the following parameters:

    Name: agent-tools
    Image Selection: Standard Data Science
    Version: 2025.1 (select the latest version)
+
Leave all the other fields as defaults. You should see the `Hardware profile` auto-selected to use the GPU Accelerator `Nvidida L4 (Shared)`.
+
image::cuda-workbench2.png[CUDA Workbench, 800]
+
Select `Create workbench`.

. Once the workbench is running open it in your browser.

=== Open the first notebook in your workbench

. Clone the code into your workbench by using the `Terminal` and entering:
+
[source,bash,options="wrap",role="execute"]
----
git clone https://github.com/redhat-ai-services/etx-agentic-ai.git
----
+
image::clone-code2.png[Clone Code, 600]

. Open up the following notebook in your workspace.
+
https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/code/getting-started.ipynb[etx-agentic-ai/code/getting-started.ipynb,window=_blank]
+

[NOTE]
====
The `getting-started.ipynb` notebook will be empty at this stage. We're including it here to demonstrate how a data scientist would typically interact with the repository and set up their environment. In later modules, you'll use this notebook as a starting point for building and testing your own agents.
====



// lightbox - for images - FIXME need to make the include::partial$lightbox.hbs WORK
++++
<div id="myModal" class="modal">
    <span class="close cursor" onclick="closeModal()">&times;</span>
    <div class="modal-content" onclick="closeModal()">
        <!--suppress HtmlRequiredAltAttribute as this will be set when selecting the image via JavaScript,
        RequiredAttributes as src will be set by when selecting the image via JavaScript -->
        <img id="imageinmodal">
    </div>
</div>
<script>
    function openModal() {
        document.getElementById("myModal").style.display = "block";
        // use overflowY = hidden to prevent the body from scrolling when modal is visible
        // doesn't work with overscroll-behavior, as this would work only when the modal has a scrollbar
        document.getElementsByTagName("body")[0].style.overflowY = "hidden";
    }

    function closeModal() {
        document.getElementById("myModal").style.display = "none";
        document.getElementsByTagName("body")[0].style.overflowY = "auto";
    }

    document.querySelectorAll('.imageblock img').forEach(element => {
        if (element.closest('a') === null) {
            element.className += " lightbox";
            element.addEventListener('click', evt => {
                document.getElementById("imageinmodal").setAttribute("src", evt.currentTarget.getAttribute("src"))
                document.getElementById("imageinmodal").setAttribute("alt", evt.currentTarget.getAttribute("alt"))
                openModal();
            })
        }
    });
</script>
<style>
    /* The Modal (background) */
    .modal {
        display: none;
        position: fixed;
        z-index: 10;
        padding-top: 5vh;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        overflow: auto;
        backdrop-filter: blur(3px);
        background-color: rgba(30, 30, 30, 0.8);
    }
    img.lightbox {
        cursor: pointer;
    }
    /* Modal Content */
    .modal-content {
        position: relative;
        margin: auto;
        padding: 0;
        width: 90%;
        max-height: 90vh;
        cursor: pointer;
    }

    .modal-content img {
        width: auto;
        height: auto;
        max-width: 90vw;
        max-height: 90vh;
        min-width: 90vw;
        min-height: 90vh;
        display: block;
        margin-right: auto;
        margin-left: auto;
        object-fit: contain;
    }

    /* The Close Button */
    .close {
        color: white;
        position: absolute;
        top: 10px;
        right: 25px;
        font-size: 35px;
        font-weight: bold;
    }

    .close:hover,
    .close:focus {
        color: #999;
        text-decoration: none;
        cursor: pointer;
    }
</style>
++++

== Objectives

By the end of this workshop, participants will have:

* âœ… A fully functional AI agent that can autonomously handle CI/CD failures
* âœ… Understanding of agent development lifecycle and deployment
* âœ… Experience with observability and evaluation of AI systems
* âœ… Practical knowledge of integrating AI agents into real-world workflows
