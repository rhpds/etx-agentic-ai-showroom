= Model Context Protocol

include::vars.adoc[]

So far, we have enabled rudimentary agentic capabilities in our playground by enabling the websearch tool, which is provided by Llama Stack as a built-in tool. For our use case, our agent also needs to interact with OpenShift and GitHub, neither of which can be done with built-in Llama Stack tools. However, Llama Stack supports usage of external tool providers, so we can leverage custom or pre-built tools that are available via https://modelcontextprotocol.io/docs/getting-started/intro[Model Context Protocol]:

[quote]
____
MCP (Model Context Protocol) is an open-source standard for connecting AI applications to external systems. Using MCP, AI applications like Claude or ChatGPT can connect to data sources (e.g. local files, databases), tools (e.g. search engines, calculators) and workflows (e.g. specialized prompts)â€”enabling them to access key information and perform tasks. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications to external systems.
____

In other words, as long as our agent runtime supports MCP, we can plug in existing MCP-based tool providers to unlock arbitrary integration capabilities. Furthermore, as per https://modelcontextprotocol.io/specification/2025-11-25[MCP specification]:

[quote]
----
MCP provides a standardized way for applications to:
  - Share contextual information with language models
  - Expose tools and capabilities to AI systems
  - Build composable integrations and workflows
----

This means the details of the provided tools (which tools, for which purpose, how to use them) are exposed directly to the language model, so there is zero configuration overhead on our side once the MCP integration is taken care of. The dynamic discoverability and exposing of contextual information is tailored for language models.

Let's now dive into how to add MCP-based tool providers to Llama Stack.

== Adding the OpenShift MCP Server to Llama Stack

For this lab, we've already deployed the pod that runs the https://github.com/containers/kubernetes-mcp-server[**OpenShift MCP server**,window=_blank] that interacts with our OpenShift cluster. Let's add it to Llama Stack as a new tool provider.

We need to edit the `run.yaml` in our `llama-stack-config` ConfigMap to add the tool runtime and tool group.

. Within the `{etx_agentic_ai_username}-llama-stack` project, click on ConfigMaps and select `llama-stack-config` file.

+
image::llamastack-configmap-location.png[LlamaStack ConfigMap location, 800]


. Click on the YAML tab.

+
image::llamastack-configmap.png[LlamaStack ConfigMap, 800]

. Add the following entry under `providers.tool_runtime`:
+
[source,yaml,options="wrap",role="execute"]
----
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
----

. Next, add the following entry under `tool_groups`:
+
[source,yaml,options="wrap",role="execute"]
----
    - toolgroup_id: mcp::openshift
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://ocp-mcp-server.mcp-openshift.svc.cluster.local:8000/sse
----
+
.Your run.yaml should now look like this.
[%collapsible]
====
[source,yaml,option="wrap"]
----
# Llama Stack configuration
version: '2'
image_name: rh
apis:
- inference
- tool_runtime
- agents
- safety
- vector_io
models:
  - metadata: {}
    model_id: granite-3-2-8b-instruct
    provider_id: vllm-granite-3-2-8b-instruct
    provider_model_id: granite-3-2-8b-instruct
    model_type: llm
providers:
  inference:
  - provider_id: vllm-granite-3-2-8b-instruct
    provider_type: "remote::vllm"
    config:
      url: https://litellm-prod.apps.maas.redhatworkshops.io/v1
      context_length: 4096
      api_token: ${env.DEFAULT_MODEL_API_TOKEN}
      tls_verify: true
  tool_runtime:
  - provider_id: tavily-search
    provider_type: remote::tavily-search
    config:
      api_key: ${env.TAVILY_API_KEY}
      max_results: 3
  - provider_id: model-context-protocol
    provider_type: remote::model-context-protocol
    config: {}
  agents:
  - provider_id: meta-reference
    provider_type: inline::meta-reference
    config:
      persistence_store:
        type: sqlite
        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/rh}/agents_store.db
      responses_store:
        type: sqlite
        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/rh}/responses_store.db
server:
  port: 8321
tools:
  - name: builtin::websearch
    enabled: true
tool_groups:
- provider_id: tavily-search
  toolgroup_id: builtin::websearch
- toolgroup_id: mcp::openshift
  provider_id: model-context-protocol
  mcp_endpoint:
    uri: http://ocp-mcp-server.mcp-openshift.svc.cluster.local:8000/sse
----
====

. Hit `Save`. The Llama Stack operator restarts Llama Stack since its configuration changed.

. Restart the playground by deleting its pod (starting with `llama-stack-playground`). Wait until it's Ready and Running.

+
image::llamastack-playground-restart.png[Llama Stack Playground restart, 800]
//TODO: update screenshot with lab user

. Refresh the playground in the browser. Enter the **Tools** playground, select the **openshift** MCP Server entry and expand `Tools from`.

+
image::llamastack-playground-mcp-openshift.png[Llama Stack MCP OpenShift tool, 800]

. Review the list of tools that can be invoked via the OpenShift tools provider.

+
image::llamastack-playground-mcp-openshift-tools.png[Llama Stack MCP OpenShift tool list, 800]

. Select **Regular agent** and try the prompt:
+
[source,bash,options="wrap",role="execute"]
----
list pods in the mcp-openshift namespace
----

. Experiment with different agents and prompts.

[TIP]
====
Not all the agent types and prompts work all of the time. This is a common problem with tool calling and LLMs. In general, larger and more capable models perform more reliably and predictably with tools.
====


== Adding the GitHub MCP Server to Llama Stack

In the previous section, we used an MCP server that was already provisioned for us. In this section, we'll:

* Enable access to a GitHub repository
* Deploy the https://github.com/github/github-mcp-server[GitHub MCP server,window=_blank]
* Enable it in Llama Stack
* Use it in the playground

First, we need to enable access to our GitHub repository so our GitHub tools can interact with it.

. **Set up your repository**
.. Fork the https://github.com/rhpds/etx-agentic-ai-gitops[lab repository,window=_blank] to your personal GitHub account.
+
.GitHub Repo Fork
image::github-fork.png[GitHub Repo Fork, 400]
+

.. Ensure that you **Enable Issues** for your fork under **Settings** > **General** > **Features** > **Issues** as they are disabled for forked repositories by default.
+
.GitHub Repo Enable Issues
image::github-repo-enable-issues.png[GitHub Repo Enable Issues, 400]
+

. **Setup GitHub Token**

.. Click on your user icon > **Settings**

.. Select **Developer Settings** > **Personal Access Tokens** > **Fine-grained personal access tokens**

.. Select Button **Generate a new token** - give it a token name e.g. __agent-lab__

.. Set **Repository access**
+
**Select repositories**: allow access to the repository you forked above.

.. Give it the following permissions:
+
**Commit statuses**: Read-Only
+
**Content**: Read-Only
+
**Issues**: Read and Write
+
**Metadata**: Read-Only (this gets added automatically)
+
**Pull requests**: Read-Only
+
.GitHub Repo Perms
image::github-repo-perms.png[GitHub Repo Perms, 400]

.. Generate the token.
+
[TIP]
====
Save the token for easy access as you'll need to use it later in the lab.
====
+

. **Deploy GitHub MCP Server**

.. In the OpenShift Console, select project `{etx_agentic_ai_username}-llama-stack`, then select `Quick create` -> `Import YAML`

+
image::ocp-import-yaml.png[OpenShift import YAML, 400]
+
.. Copy and paste the following content:

+
[source,yaml,options="wrap",role="execute"]
----
---
apiVersion: v1
kind: Secret
metadata:
  name: github-credentials-v1
stringData:
  # Add your PAT here
  token: <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN>
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: github-mcp-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: github-mcp-server
  template:
    metadata:
      labels:
        app: github-mcp-server
    spec:
      containers:
      - name: github-mcp-server
        image: quay.io/eformat/github-mcp-server:latest
        imagePullPolicy: Always
        command: ["/usr/local/bin/start-server.sh"]
        ports:
        - containerPort: 8080
        env:
        - name: GITHUB_PERSONAL_ACCESS_TOKEN
          valueFrom:
            secretKeyRef:
              name: github-credentials-v1
              key: token
        resources:
          limits:
            memory: "512Mi"
          requests:
            cpu: "200m"
            memory: "256Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: github-mcp-server
spec:
  selector:
    app: github-mcp-server
  ports:
  - port: 80
    targetPort: 8080
----
+

. Replace the token placeholder `<YOUR_GITHUB_PERSONAL_ACCESS_TOKEN>` with your GitHub personal access token you had created earlier.

. Hit `Create`.

=== **Add the GitHub tool provider to Llama Stack**

As in the previous section, we'll reference the GitHub MCP server in the Llama Stack configuration.

. Edit the LlamaStack ConfigMap `llama-stack-config`.

. Add the following entry under `tool_groups` and save your changes. Note that we don't have to edit the tool runtime section since we already added the MCP runtime entry previously.

+
[source,yaml,options="wrap",role="execute"]
----
    - toolgroup_id: mcp::github
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://github-mcp-server:80/sse
----
+
.For reference, your run.yaml should now look like this.
[%collapsible]
====
[source,yaml,option="wrap"]
----
# Llama Stack configuration
version: '2'
image_name: rh
apis:
- inference
- tool_runtime
- agents
- safety
- vector_io
models:
  - metadata: {}
    model_id: granite-3-2-8b-instruct
    provider_id: vllm-granite-3-2-8b-instruct
    provider_model_id: granite-3-2-8b-instruct
    model_type: llm
providers:
  inference:
  - provider_id: vllm-granite-3-2-8b-instruct
    provider_type: "remote::vllm"
    config:
      url: https://litellm-prod.apps.maas.redhatworkshops.io/v1
      context_length: 4096
      api_token: ${env.DEFAULT_MODEL_API_TOKEN}
      tls_verify: true
  tool_runtime:
  - provider_id: tavily-search
    provider_type: remote::tavily-search
    config:
      api_key: ${env.TAVILY_API_KEY}
      max_results: 3
  - provider_id: model-context-protocol
    provider_type: remote::model-context-protocol
    config: {}
  agents:
  - provider_id: meta-reference
    provider_type: inline::meta-reference
    config:
      persistence_store:
        type: sqlite
        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/rh}/agents_store.db
      responses_store:
        type: sqlite
        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/rh}/responses_store.db
server:
  port: 8321
tools:
  - name: builtin::websearch
    enabled: true
tool_groups:
- provider_id: tavily-search
  toolgroup_id: builtin::websearch
- toolgroup_id: mcp::openshift
  provider_id: model-context-protocol
  mcp_endpoint:
    uri: http://ocp-mcp-server.mcp-openshift.svc.cluster.local:8000/sse
- toolgroup_id: mcp::github
  provider_id: model-context-protocol
  mcp_endpoint:
    uri: http://github-mcp-server:80/sse
----
====

. Restart the playground by deleting its pod (starting with `llama-stack-playground`). Wait until it's Ready and Running.

. Refresh the playground in the browser. Select the **Tools** playground with the **github** MCP tool provider and **Regular agent**. Try the prompt (replace the GitHub user with your user).
+
[source,bash,options="wrap",role="execute"]
----
List the branches of the ${YOUR_GITHUB_USER}/etx-agentic-ai-gitops repository.
----
+
image::llama-playground-mcp-github-chat.png[LlamaStack MCP GitHub, 800]
+

. Experiment with different agents and prompts.

. Review the list of tools that can invoked via the github tool provider.

+
image::llamastack-playground-mcp-github-tools.png[LlamaStack MCP OpenShift tool list, 800]

. *Create a GitHub issue*

+
.. Try this prompt (replace `${YOUR_GITHUB_USER}` with your GitHub user ID).
+
[source,bash,options="wrap",role="execute"]
----
Create a GitHub issue for a fake error in order to demonstrate GitHub tool usage.

Use the "create_issue" tool with these tool parameters:
{"arguments": {"owner": "${YOUR_GITHUB_USER}", "repo": "etx-agentic-ai-gitops", "title": "Fake Error: Agentic AI Service Unresponsive", "body": "The Agentic AI service is not responding. This is a fake error report."}
Do not add any optional parameters.
----
+
image::playground-github-issue.png[LlamaStack Playground Github issue prompt, 800]
+

.. Confirm that the issue has been created in your repo

+
image::github-issue.png[Github issue, 800]

Great, we were able to have the LLM create a GitHub issue for us through a simple chat interface! We're now ready to move on from interactive experimentation to agent development.

== Summary

In this module, we covered the basics of Llama Stack and agent prototyping:

- We introduced Llama Stack and interacted with it via the Llama Stack Playground.
- We enabled the websearch tool in Llama Stack and used it to perform simple web search queries.
- We reviewed the basics of tool calling and Regular vs. ReAct agents.
- We introduced MCP and unlocked OpenShift and GitHub tools in Llama Stack using pre-built MCP servers.

We've now got all the pieces we need to build our CI/CD agent. Let's jump right in!