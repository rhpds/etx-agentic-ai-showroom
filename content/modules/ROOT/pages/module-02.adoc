= Using Llama Stack

include::vars.adoc[]

[NOTE]
====
Persona: AI Engineer (primary). Also relevant: Platform Engineer.
====

[IMPORTANT]
.In this section [Needs update]
====
We are going to look at the details of LlamaStack server configuration. Along the way we will use the LlamaStack playground UI as a test client and show how to configure providers, models and MCP servers.

In the final section we will use the playground to develop our Use Case specific prompts.
====

[NOTE]
====
Estimated time: 60–90 minutes
====

== What you'll do

* Interact with Llama Stack & Playground
* Register built-in/MCP tools
* Develop and refine prompts in Playground
* Capture configuration and decisions for later modules


== LlamaStack

https://llama-stack.readthedocs.io/en/latest/[LlamaStack,window=_blank] is the open-source framework for building generative AI applications. We are going to deploy LlamaStack distribution and then take a look around using the client CLI and the playground UI.

=== Interacting with LlamaStack

. Configure the **llama-stack-client** in your showroom terminal.

+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client configure

http://{etx_agentic_ai_username}-llama-stack-service.{etx_agentic_ai_username}-llama-stack.svc.cluster.local:8321

no key
----
+

. Login to OpenShift if you are not already logged in
+
[source,bash,options="wrap",role="execute"]
----
oc login --server=https://api.${CLUSTER_NAME}.${BASE_DOMAIN}:6443 -u admin -p ${ADMIN_PASSWORD}
----


. Check the connection by listing the version - ideally we match client and server versions
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client inspect version
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/version "HTTP/1.1 200 OK"
VersionInfo(version='0.2.15')
----

. If you need help with the client commands, take a look at
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client --help
----

. Now list the providers - this should match what we have configured so far i.e. Tavily Web Search, Inference and agents
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client providers list
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/providers "HTTP/1.1 200 OK"
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ API          ┃ Provider ID    ┃ Provider Type          ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩
│ inference    │ vllm           │ remote::vllm           │
│ tool_runtime │ tavily-search  │ remote::tavily-search  │
│ agents       │ meta-reference │ inline::meta-reference │
└──────────────┴────────────────┴────────────────────────┘

----

. Check the LlamaStack OpenAPI docs at http://localhost:8321/docs
+
image::llama-stack-api-docs.png[LlamaStack API Docs, 800]
+
TIP: Browsing will not work in a workbench

. Now list the models
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client models list
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models "HTTP/1.1 200 OK"

Available Models

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ model_type                   ┃ identifier                                               ┃ provider_resource_id                                     ┃ metadata               ┃ provider_id                   ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ llm                          │ granite-31-2b-instruct                                   │ granite-31-2b-instruct                                   │                        │ vllm                          │
└──────────────────────────────┴──────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────┴────────────────────────┴───────────────────────────────┘

Total models: 1

----
. Done ✅

=== Playground: LlamaStack User Interface

LlamaStack comes with a simple UI called Playground. For this lab, a Playground instance is already deployed for each user.

. Checkout the Playground pod.
+
image::llamastack-playground-pod.png[LlamaStack Playground Pod, 800]
+

. Click on the route for this pod to access the Playground.
+
image::llamastack-playground-route.png[LlamaStack Playground Route, 800]
+

. Here is the Playground, GUI for interacting with LlamaStack services.
+
image::llama-stack-playground2.png[LlamaStack Playground UI, 800]

. Let's first Chat with the LLM by asking a question such as:
+
[source,bash,options="wrap",role="execute"]
----
What is LlamaStack ?
----

+
image::llama-stack-playground-hello.png[LlamaStack Playground Hello, 800]
+

[NOTE]
Playground calls the **/v1/chat/completion** endpoint that we can find in the OpenAI docs for the vLLM served model.

. Now, let's ask LLM a question for which it lacks information (e.g. real time data):

+
[source,bash,options="wrap",role="execute"]
----
What is the weather today in Brisbane ?
----
+
image::llamastack-playground-no-websearch.png[LlamaStack Playground without websearch, 800]


. In order to find the realtime information, the LLM needs to call a tool like websearch. Let's dive into how to configure Llama Stack and set up access for the websearch tool.

=== Llama Stack Distribution

As part of OpenShift AI, a Llama Stack instance is represented by the LlamaStackDistribution custom resource (CR) that defines the Llama Stack server configuration including:

* **Server Configuration**: Which distribution to use (including built-in tools etc.)
* **Container Specifications**: Port, environment variables, resource limits

There is a unique instance of LlamaStack distribution deployed for each lab user.

. Locate the LlamaStack distribution.
+
image::llamastackdistribution-search.png[LlamaStack distribution, 800]
+

. Click on the existing distribution.
+
image::llamastackdistribution-current.png[LlamaStack distribution current, 800]
+

. Click on the YAML tab
+
image::llamastackdistribution-tab.png[LlamaStack distribution tab, 800]
+

. Check out the LlamaStack Distribution we're using for this lab.

[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: user1-llama-stack
spec:
  replicas: 1
  server:
    containerSpec:
      env:
        - name: TELEMETRY_SINKS
          value: 'console, sqlite, otel_trace, otel_metric'
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: 'http://otel-collector-collector.observability-hub.svc.cluster.local:4318'
        - name: OTEL_SERVICE_NAME
          value: user1-llamastack
        - name: DEFAULT_MODEL_API_TOKEN   # [1]
          valueFrom:
            secretKeyRef:
              key: apiKey
              name: granite-3-2-8b-instruct
        - name: TAVILY_API_KEY            # [2]
          valueFrom:
            secretKeyRef:
              key: tavily-search-api-key
              name: tavily-search-key
      name: llama-stack
      port: 8321
    distribution:
      name: rh-dev
    userConfig:                           # [3]
      configMapName: llama-stack-config
----

You will observe that we've specified the secrets for the services that'll be interacting with the Llama Stack server:

* [1] MaaS API
* [2] Tavily search API. We will update this API key later.

Also listed is [3] the user configuration used with this distribution (ConfigMap: llama-stack-config). Let's review this configuration before updating the Tavily search key.

=== LlamaStack User Config

The user config configmap can be used to store the `run.yaml` configuration for each Llama Stack distribution. Updates to the ConfigMap will restart the Pod to load the new data.

Your current Llama Stack config should like the following. Please refer to the https://llamastack.github.io/docs/distributions/customizing_run_yaml[Llama Stack documentation] for details and customization options.

[source,yaml,options="wrap"]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: llama-stack-config
data:
  run.yaml: |
    # Llama Stack configuration
    version: '2'
    image_name: rh
    apis:
    - inference
    - tool_runtime
    - agents
    - safety
    - vector_io
    models:
      - metadata: {}
        model_id: granite-3-2-8b-instruct
        provider_id: vllm-granite-3-2-8b-instruct
        provider_model_id: granite-3-2-8b-instruct
        model_type: llm
    providers:
      inference:
      - provider_id: vllm-granite-3-2-8b-instruct
        provider_type: "remote::vllm"
        config:
          url: https://litellm-prod.apps.maas.redhatworkshops.io/v1
          context_length: 4096
          api_token: ${env.DEFAULT_MODEL_API_TOKEN}
          tls_verify: true
      tool_runtime:
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_API_KEY}
          max_results: 3
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/rh}/agents_store.db
          responses_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/rh}/responses_store.db
    server:
      port: 8321
    tools:
      - name: builtin::websearch
        enabled: true
    tool_groups:
    - provider_id: tavily-search
      toolgroup_id: builtin::websearch
----

Let's now create our own Tavily key and update the Llama Stack distribution to use this key for our websearch tool calls.

**Tavily search token**: Gather your Tavily web search API Key.

. Set up a https://app.tavily.com[Tavily,window=_blank] api key for web search. Log in using a Github account of one of your team members.
+
.Tavily API Key
image::tavily-apikey.png[Create Tavily API Key, 600]

. Create a new secret object using this specification while replacing the PLACEHOLDER with your Tavily key.

[source,yaml,options="wrap"]
----
kind: Secret
apiVersion: v1
metadata:
  name: new-tavily-search-key
  namespace: {etx_agentic_ai_username}-llama-stack
stringData:
  tavily-search-api-key: PLACEHOLDER
type: Opaque
----

. Now update the Llama Stack distribution to reference this new secret instead of the old one.
+
.Updated Llama Stack Distribution
image::llamastackdistribution-updated-key.png[Updated Llama Stack config, 600]

. Let's now try out the websearch tool! Navigate back to the Llama Stack Playground UI.

. In the left menu, choose `Tools`. Select the built-in `websearch` tool. We also need to specify what type of agent to call. Let's select the **Regular** agent to call the tool.

+
image::llamastack-playground-websearch.png[LlamaStack Playground with websearch, 800]

. Now, let's ask LLM the same question again:

+
[source,bash,options="wrap",role="execute"]
----
What is the weather today in Brisbane ?
----
The LLM is now able to the answer with realtime data provided by the websearch tool.

+
image::llamastack-playground-websearch-regular.png[LlamaStack Playground regular websearch, 800]

+
In summary, here is the workflow with a Regular agent.
+
image::agent.png[Agent workflow, 600]



. Now lets try out the **ReAct** agent to call the tool with the same prompt:
+
[source,bash,options="wrap",role="execute"]
----
What is the weather today in Brisbane ?
----
+
Notice that the agent first Reasons - where the LLM thinks about the data or tool results, Acts - where the LLM performs an action, LLM then Observes the result of the tool call, before returning the result.
+
image::llamastack-playground-websearch-react.png[LlamaStack Websearch ReAct Agent, 800]

+
In summary, here is the workflow with a ReAct agent.
+
image::react-agent.png[ReAct Agent, 600]

. Done ✅



=== LlamaStack mcp::openshift

Model Context Protocol (MCP) is a popular standard for tool discovery and execution. It allows tools to be dynamically discovered from an MCP endpoint and can be used to extend the agent’s capabilities.

For this lab, we've already deployed the pod that runs the https://github.com/containers/kubernetes-mcp-server[**mcp::openshift**,window=_blank] functions that interact with the OpenShift cluster. However, we need to configure LlamaStack to use this MCP tool.


We need to edit the LlamaStack ConfigMap to add tool runtime and tool group.

. Search for your namespace ({etx_agentic_ai_username}-llama-stack), click on ConfigMaps and select llama-stack-config file.

+
image::llamastack-configmap-location.png[LlamaStack ConfigMap location, 800]


. Click on YAML tab.

+
image::llamastack-configmap.png[LlamaStack ConfigMap, 800]

. Now, we'll add the tool runtime for MCP:
+
[source,yaml,options="wrap"]
----
      tool_runtime:
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
----

. Next, we will add the tool group
+
[source,yaml,options="wrap"]
----
    tool_groups:
    - toolgroup_id: mcp::openshift
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://ocp-mcp-server.mcp-openshift.svc.cluster.local:8000/sse
----

. Refresh the playground in the browser. Select the **Tools** playground with the **MCP Servers openshift** and expand tools from.

+
image::llamastack-playground-mcp-openshift.png[LlamaStack MCP OpenShift tool, 800]

. Checkout the list of actions that can performed by MCP:OpenShift tools.

+
image::llamastack-playground-mcp-openshift-tools.png[LlamaStack MCP OpenShift tool list, 800]


. Select **Regular agent** and try the prompt:
+
[source,bash,options="wrap",role="execute"]
----
list pods in the mcp-openshift namespace
----

. Try different agents and prompts. Not all of them work all of the time. This is a common problem with Tool calling and LLMs.

. Done ✅

=== LlamaStack mcp::github


In the previous section, we used MCP servers that were already provisioned for us. In this section, we'll:

* Enable access to GitHub repo
* Deploy https://github.com/github/github-mcp-server[GitHub server,window=_blank]
* Enable it in LlamaStack
* Use it in the Playground

First, we need to enable access to our github repo so MCP GitHub tool can interact with it.

. **Set up your repository**
.. https://github.com/rhpds/etx-agentic-ai-gitops[Fork the etx-agentic-ai repository,window=_blank] to your personal GitHub account
+
.GitHub Repo Fork
image::github-fork.png[GitHub Repo Fork, 400]
+

.. Ensure that you **Enable Issues** for your fork under **Settings** > **General** > **Features** > **Issues** as they are disabled for forked repos by default
+
.GitHub Repo Enable Issues
image::github-repo-enable-issues.png[GitHub Repo Enable Issues, 400]
+

. **Setup GitHub Token**

.. Click on your user icon > **Settings**

.. Select **Developer Settings** > **Personal Access Tokens** > **Fine-grained personal access tokens**

.. Select Button **Generate a new token** - give it a token name e.g. __etx-ai__

.. Set **Repository access**
+
**Select repositories**: allow access to the repository you forked above.

.. Give it the following permissions:
+
**Commit statuses**: Read-Only
+
**Content**: Read-Only
+
**Issues**: Read and Write
+
**Metadata**: Read-Only (this gets added automatically)
+
**Pull requests**: Read-Only
+
.GitHub Repo Perms
image::github-repo-perms.png[GitHub Repo Perms, 400]

.. Generate the token.
+
.GitHub Repo Token
image::github-pat.png[GitHub Repo Token, 400]

[TIP]
====
Save the token for easy access as you'll need to use it later in the lab.
====


. **Clone the repository**
.. If you're running the lab using your laptop, you can clone the repository locally to your laptop.
+
[source,bash,options="wrap",role="execute"]
----
git clone git@github.com:your-gh-user/etx-agentic-ai-gitops.git
cd etx-agentic-ai-gitops
----
+
image::github-clone.png[GitHub Repo Clone, 400]
+


.. Done ✅

. **Deploy MCP GitHub Server**

.. In the OpenShift Console, choose project ({etx_agentic_ai_username}.llama-stack) and navigate to Workloads -> Secrets.

.. Click on *Create* (from YAML)

+
[source,yaml,options="wrap"]
----
apiVersion: v1
kind: Secret
metadata:
  name: github-credentials-v1
  namespace: {etx_agentic_ai_username}-llama-stack
type: Opaque
stringData:
  # Add your PAT here
  # Important: not for production use, demo purposes only
  token: <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN>
----
+

[TIP]
====
Update namespace and your GitHub personal access token you had created earlier
====

.. In the OpenShift Console, continuing with project {etx_agentic_ai_username}.llama-stack), navigate to Workloads -> Deployments.

.. Click on *Create* and choose YAML view

+
[source,yaml,options="wrap"]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: github-mcp-server
  namespace: {etx_agentic_ai_username}-llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: github-mcp-server
  template:
    metadata:
      labels:
        app: github-mcp-server
    spec:
      containers:
      - name: github-mcp-server
        image: quay.io/eformat/github-mcp-server:latest
        imagePullPolicy: Always
        command: ["/usr/local/bin/start-server.sh"]
        ports:
        - containerPort: 8080
        env:
        - name: GITHUB_PERSONAL_ACCESS_TOKEN
          valueFrom:
            secretKeyRef:
              name: github-credentials-v1
              key: token
        resources:
          limits:
            memory: "512Mi"
          requests:
            cpu: "200m"
            memory: "256Mi"
----
+

[TIP]
====
Update namespace
====


.. In the OpenShift Console, continuing with project {etx_agentic_ai_username}.llama-stack), navigate to Networking -> Services.

.. Click on *Create Service*

+
[source,yaml,options="wrap"]
----
apiVersion: v1
kind: Service
metadata:
  name: github-mcp-server
  namespace: {etx_agentic_ai_username}-llama-stack
spec:
  selector:
    app: github-mcp-server
  ports:
  - port: 80
    targetPort: 8080
----
+

[TIP]
====
Update namespace
====

+
. **Enable mcp::github in LlamaStack**

[NOTE]
====
The configuration is very similar to what we did for add MCP:OpenShift tool earlier.
====

. Edit the LlamaStack ConfigMap to add MCP:GitHub tool.

. Add in the tool group.

+
[source,yaml,options="wrap"]
----
    tool_groups:
    - toolgroup_id: mcp::github
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://github-mcp-server:80/sse
----
+

[NOTE]
====
Notice that the URI for the MCP Server uses Server Sent Events (/sse).
====

. Refresh the playground in the browser. Select the **Tools** playground with the **MCP Servers github** and **Regular agent**. Try the prompt (replace the github user with your user).
+
[source,bash,options="wrap",role="execute"]
----
List the branches from ${YOUR_GITHUB_USER}/etx-agentic-ai-gitops repo.
----
+
image::llama-playground-mcp-github-chat.png[LlamaStack MCP GitHub, 800]
+

. Try different agents and prompts. Not all of them work all of the time. This is a common problem with Tool calling and LLMs

. Checkout the list of actions that can performed by MCP:github tools.

+
image::llamastack-playground-mcp-github-tools.png[LlamaStack MCP OpenShift tool list, 800]

. *Create a GitHub issue*

+
.. Try this prompt (replace the github user with your user).
+
[source,bash,options="wrap",role="execute"]
----
Create a github issue for a fake error in the ${YOUR_GITHUB_USER}/etx-agentic-ai repo and assign it to ${YOUR_GITHUB_USER}.
----
+
image::playground-github-issue.png[LlamaStack Playground Github issue prompt, 800]
+

.. Confirm that the issue has been created in your repo

+
image::github-issue.png[Github issue, 800]
+

. Done ✅


=== LlamaStack Observability

LlamaStack integrates with the Observability stack we deployed as part of the bootstrap. The observability stack has a lot of moving parts. Traces are sent from LlamaStack via OTEL to a Tempo sink endpoint. We can then view traces in OpenShift using the Observe > Traces dashboard.

. Edit the LlamaStack ConfigMap to add telemetry stanza with the service name, sinks and the OTEL Tracing endpoint which is set as an environment variable on the Deployment
+
[source,yaml,options="wrap"]
----
apis:
...
- telemetry

providers:
...
  telemetry:
  - provider_id: meta-reference
    provider_type: inline::meta-reference
    config:
      service_name: "${env.OTEL_SERVICE_NAME:=}"
      sinks: ${env.TELEMETRY_SINKS:=console}
      otel_exporter_otlp_endpoint: ${env.OTEL_EXPORTER_OTLP_ENDPOINT:=}
      sqlite_db_path: /opt/app-root/src/.llama/distributions/rh/trace_store.db
----

. Refresh the playground in the browser. Select the Tools playground and select websearch, the MCP github Server and MCP OpenShift tool, ReAct agent and Llama4 model. Try out some of the previous prompting that include Tool calls to generate some traces.
+
image::llama-stack-traces1.png[LlamaStack Traces, 800]
+
image::llama-stack-traces2.png[LlamaStack Traces, 800]

. Done ✅

=== LlamaStack Configured

. Check the completed LlamaStack configuration
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client providers list
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/providers "HTTP/1.1 200 OK"
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ API          ┃ Provider ID            ┃ Provider Type                  ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ scoring      │ basic                  │ inline::basic                  │
│ scoring      │ llm-as-judge           │ inline::llm-as-judge           │
│ agents       │ meta-reference         │ inline::meta-reference         │
│ inference    │ vllm                   │ remote::vllm                   │
│ inference    │ vllm-llama-3-2-3b      │ remote::vllm                   │
│ inference    │ vllm-llama-4-guard     │ remote::vllm                   │
│ inference    │ sentence-transformers  │ inline::sentence-transformers  │
│ tool_runtime │ model-context-protocol │ remote::model-context-protocol │
│ tool_runtime │ brave-search           │ remote::brave-search           │
│ tool_runtime │ tavily-search          │ remote::tavily-search          │
│ telemetry    │ meta-reference         │ inline::meta-reference         │
└──────────────┴────────────────────────┴────────────────────────────────┘
----
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client models list
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models "HTTP/1.1 200 OK"

Available Models

┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ model_type      ┃ identifier                                                         ┃ provider_resource_id                    ┃ metadata                                    ┃ provider_id                  ┃
┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ llm             │ vllm/granite-31-2b-instruct                                        │ granite-31-2b-instruct                  │                                             │ vllm                         │
├─────────────────┼────────────────────────────────────────────────────────────────────┼─────────────────────────────────────────┼─────────────────────────────────────────────┼──────────────────────────────┤
│ llm             │ vllm-llama-3-2-3b/llama-3-2-3b                                     │ llama-3-2-3b                            │                                             │ vllm-llama-3-2-3b            │
├─────────────────┼────────────────────────────────────────────────────────────────────┼─────────────────────────────────────────┼─────────────────────────────────────────────┼──────────────────────────────┤
│ llm             │ vllm-llama-4-guard/llama-4-scout-17b-16e-w4a16                     │ llama-4-scout-17b-16e-w4a16             │                                             │ vllm-llama-4-guard           │
├─────────────────┼────────────────────────────────────────────────────────────────────┼─────────────────────────────────────────┼─────────────────────────────────────────────┼──────────────────────────────┤
│ embedding       │ sentence-transformers/all-MiniLM-L6-v2                             │ all-MiniLM-L6-v2                        │ {'embedding_dimension': 384.0}              │ sentence-transformers        │
└─────────────────┴────────────────────────────────────────────────────────────────────┴─────────────────────────────────────────┴─────────────────────────────────────────────┴──────────────────────────────┘

Total models: 4
----

. Done ✅
