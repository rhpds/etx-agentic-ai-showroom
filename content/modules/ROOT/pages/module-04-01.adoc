:imagesdir: ../assets/images
= Adding Guardrails

In the context of AI, "Guardrails" come in many forms. Guardrails are safety mechanisms that ensure your AI agents behave responsibly, securely, and within acceptable boundaries. They act as protective barriers against unintended behaviors, security vulnerabilities, and misuse.

Some common examples include:

== Input Guardrails

* **Prompt Injection Prevention**: Detect and block attempts to manipulate the agent through malicious prompts that try to override system instructions or extract sensitive information
* **Content Filtering**: Screen user inputs for inappropriate content, profanity, hate speech, or personally identifiable information (PII) before processing
* **Input Validation**: Ensure user queries conform to expected formats and don't contain malicious payloads (SQL injection, command injection, etc.)
* **Rate Limiting**: Prevent abuse by limiting the number of requests per user or session within a time window

== Output Guardrails

* **Content Moderation**: Filter agent responses to prevent generating harmful, biased, or inappropriate content
* **PII Redaction**: Automatically detect and remove sensitive information (social security numbers, credit cards, passwords) from responses
* **Hallucination Detection**: Identify when the model generates false or ungrounded information, especially for factual queries
* **Toxicity Filtering**: Prevent the agent from generating offensive, discriminatory, or harmful language

== Behavioral Guardrails

* **Tool Usage Restrictions**: Limit which tools the agent can invoke and under what conditions (e.g., prevent destructive operations without confirmation)
* **Action Approval Workflows**: Require human-in-the-loop approval for high-risk actions like deleting resources, modifying production systems, or accessing sensitive data
* **Scope Boundaries**: Restrict the agent to operate only within designated namespaces, repositories, or resource boundaries
* **Cost Controls**: Monitor and limit token usage, API calls, or computational resources to prevent runaway costs

== Compliance and Safety Guardrails

* **Regulatory Compliance**: Ensure responses comply with regulations like GDPR, HIPAA, or industry-specific standards
* **Brand Safety**: Prevent the agent from making commitments, promises, or statements that could create legal or reputational risks
* **Audit Logging**: Record all agent interactions, decisions, and tool invocations for compliance and incident investigation
* **Jailbreak Detection**: Identify and block sophisticated attempts to circumvent safety measures through multi-turn conversations or encoded instructions

== Implementing Guardrails in Llama Stack

When you are handling many different Guardrails, you need to consider orchestration. This is where the https://trustyai.org/docs/main/gorch-tutorial[Trusty AI Guardrails Orchestrator] comes into play.

Today, we will only implement one type of Guardrail: the https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/[Llama Guard] model. This model is specifically targeted to prevent harmful content.

Llama Stack provides built-in support for safety guardrails through its safety API. These can be configured at different layers of your agent system to provide defense-in-depth protection.

Edit the ConfigMap:

// Information to add to Guardrail just waiting on what I need to default include in gitops.

=== Test Guardrail functionality in the Llama Stack playground. 

. Go back to the playground interface

. Select `Agent-based`

. Add the Llama Guard model to the Input and Output sheild form sections:

// add picture when working

. Test functionality by typing something or asking something harmful in the chat interface.

== Summary

Implementing guardrails is essential for production AI deployments. They provide defense-in-depth protection, ensuring your agents:

* Cannot be manipulated through malicious prompts
* Do not generate harmful, inappropriate, or legally risky content
* Operate only within authorized boundaries
* Comply with regulatory and organizational policies
* Maintain audit trails for accountability

With guardrails in place, you can confidently deploy AI agents knowing they have protective barriers against common security and safety risks.