= Introducing Llama Stack

include::vars.adoc[]

In order to build and run our CI/CD agent, we'll need a number of components. Here is what is needed and what we'll use in our lab:

* An **LLM** for analyzing the CI/CD pipeline logs and generating recommendations for troubleshooting. We'll leverage a Granite model that is hosted in a remote environment and that we're accessing through a Models-as-a-Service interface.
* A **graphical user interface** for interactively querying the LLM and experimenting with user prompts and agentic capabilities. We'll use Llama Stack Playground, a frontend application designed for Llama Stack (see below).
* An **agentic framework** for building out the agent as an application component. There's a wide variety of established frameworks. In our lab, we'll rely on the Llama Stack Python library.
* Access to **third-party systems** depending on the use case:
** The CI/CD runtime, which contains the logs of the failed pipelines. In our lab, we'll leverage OpenShift Pipelines as the CI/CD engine. We can access the pipeline logs simply via the OpenShift interface.
** The internet for searching potential resolutions of the encountered pipelines failures.
** The destination of the generated failure report. In our case, the agent will create issues within a specific Github repository and upload its reports there.
* In order to orchestrate the various interactions between the agent, the model, and the third-party systems, we'll rely on a central **API server** that is tailored for agentic use cases. This is where **Llama Stack** comes into play.

In this chapter we will cover agent prototyping with Llama Stack. Specifically, we will

1. use Llama Stack Playground for basic prompting against our model through Llama Stack,
2. configure the built-in websearch tool in Llama Stack,
3. set up integration with OpenShift and Github through MCP,
4. build a prompt that covers the end-to-end use case.


== Llama Stack

https://llama-stack.readthedocs.io/en/latest/[Llama Stack,window=_blank] is an open-source framework and API layer for building and running generative AI applications. It provides a unified set of APIs for inference, RAG, agents, tools, safety, evals, and telemetry. Llama Stack is one of the components included in OpenShift AI.

There's a Llama Stack instance that's running in your lab environment. We will use this instance

* from this showroom through the Llama Stack CLI client,
* interactively from the Llama Stack Playground frontend,
* programmatically from the IDE and agent service through the Llama Stack Python SDK. Note that--since Llama Stack implements the OpenAI API--any library that consumes the OpenAI API can be used as a client.

=== Using the Llama Stack CLI

. Configure the **llama-stack-client** in your showroom terminal.

+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client configure
----
+
For the endpoint of the Llama Stack distribution server enter:
+
[source,bash,options="wrap",role="execute",subs="attributes+"]
----
http://{etx_agentic_ai_username}-llama-stack-service.{etx_agentic_ai_username}-llama-stack.svc.cluster.local:8321
----
+
When prompted for the API key, leave the value empty and hit enter.


. Check the connection by listing the version--ideally we match client and server versions
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client inspect version
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/version "HTTP/1.1 200 OK"
VersionInfo(version='0.2.22')
----

. If you need help with the client commands, take a look at
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client --help
----

. Now list the providers--this should match what we have configured so far i.e. Tavily Web Search, inference and agents
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client providers list
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/providers "HTTP/1.1 200 OK"
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ API          ┃ Provider ID    ┃ Provider Type          ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩
│ inference    │ vllm           │ remote::vllm           │
│ tool_runtime │ tavily-search  │ remote::tavily-search  │
│ agents       │ meta-reference │ inline::meta-reference │
└──────────────┴────────────────┴────────────────────────┘

----

. Now list the models
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client models list
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models "HTTP/1.1 200 OK"

Available Models

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ model_type                   ┃ identifier                                               ┃ provider_resource_id                                     ┃ metadata               ┃ provider_id                   ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ llm                          │ granite-31-2b-instruct                                   │ granite-31-2b-instruct                                   │                        │ vllm                          │
└──────────────────────────────┴──────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────┴────────────────────────┴───────────────────────────────┘

Total models: 1

----
. Done ✅

=== Playground: Llama Stack User Interface

Llama Stack comes with a simple UI called Playground. For this lab, a Playground instance is already deployed for each user.

Select the *{etx_agentic_ai_username}-llama-stack* namespace.

. Checkout the Playground pod.
+
image::llamastack-playground-pod.png[Llama Stack Playground Pod, 800]
+

. Click on the route for this pod to access the Playground.
+
image::llamastack-playground-route.png[Llama Stack Playground Route, 800]
+

. Here is the Playground, GUI for interacting with Llama Stack services.
+
image::llama-stack-playground2.png[Llama Stack Playground UI, 800]

. Let's first chat with the LLM by asking a question such as:
+
[source,bash,options="wrap",role="execute"]
----
What is AI?
----

+
image::llama-stack-playground-hello.png[Llama Stack Playground Hello, 800]
+

. Now let's ask the LLM a question for which it lacks information (e.g. real-time data):

+
[source,bash,options="wrap",role="execute"]
----
What is the weather today in Brisbane?
----
+
image::llamastack-playground-no-websearch.png[LlamaStack Playground without websearch, 800]


. In order to find this type of real-time information, the LLM needs to call a tool like __websearch__. Let's dive into how to configure Llama Stack and set up access for the websearch tool.
