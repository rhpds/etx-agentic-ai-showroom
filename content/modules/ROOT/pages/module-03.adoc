= From Playground to Prototype - Building Your Agent in OpenShift AI

include::vars.adoc[]

[NOTE]
====
Persona: AI Engineer (primary). Also relevant: Data Scientist.
====

[IMPORTANT]
.In this lab
====
You will open an OpenShift AI Workbench, carry over decisions and artifacts from the Llama Stack Playground work in xref:module-04.adoc[the previous module], then iteratively step through `notebooks/agent-prototyping.ipynb` to codify a working agentic flow. The lab ends by creating a GitHub issue in your repository from the agent’s final output. The following lesson will take this prototype to production.
====

[NOTE]
====
Estimated time: 60–90 minutes
====

== Prerequisites

// * Completed xref:module-00.adoc[environment setup] (cluster access, repo fork, tokens)
// * Completed xref:module-04.adoc[Llama Stack] (server available, prompts tested in Playground)
// * Fine-grained GitHub PAT from xref:module-04.adoc[previous module]

[TIP]
====
Have these ready from your Playground session:

* Chosen model and endpoint (Llama Stack or MaaS)
* The refined system prompt and example user prompts
* Any tool choices (e.g., web search/Tavily) and API keys
* Parameters you validated
====

.Security reminders
* Keep PAT scopes minimal; store tokens securely
* Do not commit secrets or tokens to notebooks or repo; use pre-commit hooks to check for secrets
* Prefer Vault-managed secrets where possible

== Open an OpenShift AI Workbench

. In the OpenShift console, switch to your team namespace/project (e.g., `agent-demo`).
. Create a Workbench with a CUDA-enabled image and a GPU or appropriate CPU flavor as provided by your cluster administrator.
. Start the Workbench server and open JupyterLab.
. In a terminal inside JupyterLab, clone your fork (or pull latest):

[source,bash,options="wrap",role="execute"]
----
cd ${HOME}
git clone https://github.com/redhat-ai-services/etx-agentic-ai.git
cd etx-agentic-ai
----

[NOTE]
====
If you use a persistent volume, your repo may already exist. Ensure it’s up to date before proceeding.
====

== Load the Agent Notebook

. In JupyterLab, open `notebooks/agent-prototyping.ipynb`.
. We will execute cells top-to-bottom, pausing at each to reflect how it builds on the prior step and prepares the next.

== Configure Notebook Environment (matches notebook)

. Login to OpenShift if you are not already logged in
+
[source,bash,options="wrap",role="execute"]
----
oc login --server=https://api.${CLUSTER_NAME}.${BASE_DOMAIN}:6443 -u admin -p ${ADMIN_PASSWORD}
----

. First, expose the Llama Stack service to your Workbench via a port-forward:
+
[source,bash,options="wrap",role="execute"]
----
# From a Workbench Terminal
oc project llama-stack
oc get svc llamastack-with-config-service
----
+
[source,bash,options="wrap",role="execute"]
----
oc -n llama-stack port-forward svc/llamastack-with-config-service 8321:8321 2>&1>/dev/null &
----
+
[NOTE]
====
This keeps a local tunnel from your Workbench pod to the service on port 8321. Leave it running for the duration of the lab. If your Llama Stack is configured with TLS, you can still use port-forward; adjust the `LLAMA_STACK_URL` scheme to `https` accordingly.
====

[TIP]
====
Reset/retry tips:

* If `! pip install -qr requirements.txt` upgrades dependencies, restart the Jupyter kernel
* Stop any existing port-forward (`Ctrl+C`) before starting a new one
====

. Apply required RBAC so the MCP server can read Tekton pipeline pods in `demo-pipeline`:
+
[source,bash,options="wrap",role="execute"]
----
oc apply -f infra/tenants/demo-pipeline/base/sno/templates/agent-rolebinding.yaml | cat
----

. Change directory to the `notebooks` folder:
+
[source,bash,options="wrap",role="execute"]
----
cd notebooks
----

. Edit the `env` if necessary to match your Playground configuration and deployment settings.
+
[IMPORTANT]
====
The notebook loads this file directly with `load_dotenv('env')`. Name the file exactly `env` (no dot). If you prefer `.env`, change the notebook call to `load_dotenv('.env')`.
====

. In JupyterLab, open the `agent-prototyping.ipynb` notebook file to begin your work.

== Step Through the Notebook Iteratively

Use the following guideposts as you run each section. Run all cells in a section before proceeding to the next.

=== Section 1: Getting Started with Llama Stack

* Run (CTRL+ENTER): `! pip install -qr requirements.txt` to install dependencies.
* Run: The imports cell.
* Run: `load_dotenv('env')` to load `LLAMA_STACK_URL`, `LLM_MODEL_ID`, etc.
* Run: The server connection cell to create `LlamaStackClient`, list models, and print `model_id`.
* Run: The sampling params cell to set `temperature`, `max_tokens`, `sampling_params`.
* Run: The quick chat completion sanity check.

* Expected: Model list printed, sampling params displayed, and a reply from the model.

=== Section 2: Simple Agent with Tool Calling

* Run: Define `Agent` with `tools=['builtin::websearch']` and `instructions`.
* Run: Define `run_session(...)` helper.
* Run: Provide `user_prompts` (e.g., latest OpenShift version) and call `run_session(...)`.

* Expected: Streamed logs from `EventLogger` and a final answer using web search.

Custom client tool (Kubernetes logs):

* Run: Kubernetes client setup (`load_incluster_config()`, `CoreV1Api`).
* Gather targets and set variables:
** Option A (Web Console): Switch to the `demo-pipeline` project. Navigate to Pipelines > PipelineRuns. Open the most recent failed run; from its details, click the Pod name for the failed Task to view its Pod. Copy the Pod name and container name.
** Option B (Terminal): Use the `demo-pipeline` namespace and find the most recent failed PipelineRun and its failed Task’s Pod:
+
[source,bash,options="wrap",role="execute"]
----
# list pods in your namespace
oc -n <your-namespace> get pods

# show container names for a specific pod
oc -n <your-namespace> get pod <pod-name> -o jsonpath='{.spec.containers[*].name}{"\n"}'
----
** Then set in the notebook:
+
[source,python]
----
pod_name = "<pod-name>"
namespace = "<your-namespace>"
container_name = "<container-name>"
----
* Run: `get_pod_log_test(...)` to verify access.
* Run: Define `@client_tool get_pod_log(...)`.
* Run: Agent using `tools=[get_pod_log]` and analyze logs with `run_session(...)`.

TIP: If a 403 Forbidden occurs when fetching logs, the PipelineRun may have already completed or permissions aren’t applied. Re-run the pipeline in `demo-pipeline` (from Pipelines UI or `tkn` CLI), wait for it to fail, then re-run the notebook cells for log retrieval.

* Expected: Log text returned and summarized by the agent.

=== Section 3: Prompt Chaining

* Run: Define agent with `tools=[get_pod_log, 'builtin::websearch']`.
* Run: The chained `user_prompts` and `run_session(...)`.

* Expected: Fetches logs → web search → summarized recommendations.

=== Section 4: ReAct

* Run: Define `ReActAgent` with `tools=[get_pod_log, 'builtin::websearch']` and `response_format` using `ReActOutput.model_json_schema()`.
* Run: The `user_prompts` and `run_session(...)`.

* Expected: Reason→Act loops with dynamic tool selection.

=== Section 5: MCP Tools and Full Flow (OpenShift + Web + GitHub)

* Run: Validate/auto-register MCP tools (e.g., `mcp::openshift`).
* Run: Define full ReAct agent using `tools=["mcp::openshift", "builtin::websearch", "mcp::github"]`.

* Edit: In the provided prompt, replace the repo owner with your fork (`"owner":"your-gh-user","repo":"etx-agentic-ai"`).
+
image::full-react-agent-repo-owner.png[Change the GitHub repo owner from `redhat-ai-services` to your GitHub username, 700]
* Run: Execute `run_session(...)` to analyze logs → search → create GitHub issue.

* Expected: Issue is created by the agent; capture the URL from the output.

=== Optional: Persist Run Artifacts

* Save a small report with inputs, parameters, and outputs so it can be attached to an issue.

[source,python]
----
import json, pathlib, time
from os import environ

report = {
    "timestamp": int(time.time()),
    "model": environ.get("LLM_MODEL_ID"),
    "endpoint": environ.get("LLAMA_STACK_URL"),
    "sampling_params": {
        "temperature": environ.get("TEMPERATURE"),
        "max_tokens": environ.get("MAX_TOKENS"),
    },
    "task": "<your final user task>",
    "final_answer": "<paste the agent’s final answer or summary>",
}
pathlib.Path("artifacts").mkdir(exist_ok=True)
with open("artifacts/agent_run_report.json", "w") as f:
    json.dump(report, f, indent=2)
print("Saved artifacts/agent_run_report.json")
----

== Verify the GitHub Issue (created by the agent)

image::github-issue-created.png[Example of a GitHub issue created by the agent’s full ReAct flow, 800]

The full MCP-based ReAct run should create the issue automatically via the GitHub MCP server. Capture the URL from the streamed logs or agent output and record it in your lab notes.

== Validation Checklist

* Workbench server is running; repo is up to date
* Notebook executed end-to-end with no unresolved errors
* `artifacts/agent_run_report.json` exists and summarizes the run
* GitHub issue created; URL recorded

== Artifacts to carry forward

* `notebooks/env` values used (LLAMA_STACK_URL, LLM_MODEL_ID, TEMPERATURE, MAX_TOKENS)
* The final agent prompt and tool choices (builtin::websearch, mcp::openshift, mcp::github)
* `artifacts/agent_run_report.json` (if created)
* URL of the created GitHub issue

== Commit Your Work

[source,bash,options="wrap",role="execute"]
----
git add notebooks/agent-prototyping.ipynb artifacts/agent_run_report.json || true
git commit -m "lab: agent prototype run artifacts"
git push
----

== What’s Next

Great work—your agent prototype is now codified and traceable via a GitHub issue. In the next lesson, we’ll take this into production: xref:module-04.adoc[Rolling out the Agent].


// lightbox - for images - FIXME need to make the include::partial$lightbox.hbs WORK
++++
<div id="myModal" class="modal">
    <span class="close cursor" onclick="closeModal()">&times;</span>
    <div class="modal-content" onclick="closeModal()">
        <!--suppress HtmlRequiredAltAttribute as this will be set when selecting the image via JavaScript,
        RequiredAttributes as src will be set by when selecting the image via JavaScript -->
        <img id="imageinmodal">
    </div>
</div>
<script>
    function openModal() {
        document.getElementById("myModal").style.display = "block";
        // use overflowY = hidden to prevent the body from scrolling when modal is visible
        // doesn't work with overscroll-behavior, as this would work only when the modal has a scrollbar
        document.getElementsByTagName("body")[0].style.overflowY = "hidden";
    }

    function closeModal() {
        document.getElementById("myModal").style.display = "none";
        document.getElementsByTagName("body")[0].style.overflowY = "auto";
    }

    document.querySelectorAll('.imageblock img').forEach(element => {
        if (element.closest('a') === null) {
            element.className += " lightbox";
            element.addEventListener('click', evt => {
                document.getElementById("imageinmodal").setAttribute("src", evt.currentTarget.getAttribute("src"))
                document.getElementById("imageinmodal").setAttribute("alt", evt.currentTarget.getAttribute("alt"))
                openModal();
            })
        }
    });
</script>
<style>
    /* The Modal (background) */
    .modal {
        display: none;
        position: fixed;
        z-index: 10;
        padding-top: 5vh;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        overflow: auto;
        backdrop-filter: blur(3px);
        background-color: rgba(30, 30, 30, 0.8);
    }
    img.lightbox {
        cursor: pointer;
    }
    /* Modal Content */
    .modal-content {
        position: relative;
        margin: auto;
        padding: 0;
        width: 90%;
        max-height: 90vh;
        cursor: pointer;
    }

    .modal-content img {
        width: auto;
        height: auto;
        max-width: 90vw;
        max-height: 90vh;
        min-width: 90vw;
        min-height: 90vh;
        display: block;
        margin-right: auto;
        margin-left: auto;
        object-fit: contain;
    }

    /* The Close Button */
    .close {
        color: white;
        position: absolute;
        top: 10px;
        right: 25px;
        font-size: 35px;
        font-weight: bold;
    }

    .close:hover,
    .close:focus {
        color: #999;
        text-decoration: none;
        cursor: pointer;
    }
</style>
++++

== From Prototype to App: Deploying the Agent as a Service

[NOTE]
====
Persona: Platform Engineer (primary). Also relevant: AI Engineer.
====

[IMPORTANT]
.In this lab
====
You will turn the agent you prototyped in the previous module into a running service on OpenShift. We will deploy the `ai-agent` tenant, build and expose the FastAPI app, and wire the Tekton pipeline trigger so failures send context to the agent which then creates a GitHub issue.
====

[NOTE]
====
Estimated time: 60–90 minutes
====

== Objectives

* Deploy the `ai-agent` tenant resources
* Point build configs to your GitHub fork
* Build and roll out the agent service (FastAPI)
* Verify connectivity to Llama Stack from the app
* Wire the pipeline trigger so failures call the agent
* Validate end-to-end by creating a GitHub issue from a failure

== Prerequisites

// * Completed xref:module-05.adoc[Module 05] with a working agent flow in the notebook
* Your repo fork is up to date and the cluster GitOps points to it
* Llama Stack server reachable inside the cluster (module 04 deployment)

== Deploy the ai-agent Tenant

* Confirm the project exists and resources are present:
+
[source,bash,options="wrap",role="execute"]
----
oc get ns ai-agent
oc -n ai-agent get deploy,svc,route,cm,sa,rolebinding,secret
----

* If resources are missing, ensure your GitOps/App-of-Apps includes the tenant. Then sync in Argo CD.

== Point to Your GitHub Fork and Correct the Webhook Path

By default the tenant and Helm chart reference `redhat-ai-services/etx-agentic-ai` and post to `/webhook`, while the FastAPI app exposes `/report-failure`.

* Set your GitHub username:
+
[source,bash,options="wrap",role="execute"]
----
export YOUR_GITHUB_USER=your-gh-user
export YOUR_FORK=https://github.com/${YOUR_GITHUB_USER}/etx-agentic-ai.git
----

* Update the tenant Tekton Pipeline to use your fork and the correct endpoint:
+
[source,bash,options="wrap",role="execute"]
----
sed -i "s#https://github.com/redhat-ai-services/etx-agentic-ai.git#${YOUR_FORK}#g" \
  infra/tenants/ai-agent/base/sno/templates/pipeline.yaml

# Use the app's actual endpoint
sed -i "s#/webhook#/report-failure#g" \
  infra/tenants/ai-agent/base/sno/templates/pipeline.yaml
----

* Update the Job that triggers a PipelineRun so it also uses your fork:
+
[source,bash,options="wrap",role="execute"]
----
sed -i "s#https://github.com/redhat-ai-services/etx-agentic-ai.git#${YOUR_FORK}#g" \
  infra/tenants/ai-agent/base/sno/templates/job.yaml
----

* (Optional, if using the Helm chart locally) Update the chart defaults:
+
[source,bash,options="wrap",role="execute"]
----
sed -i "s#https://github.com/redhat-ai-services/etx-agentic-ai.git#${YOUR_FORK}#g" code/chart/values.yaml
sed -i "s#/webhook#/report-failure#g" code/chart/values.yaml
----

* Commit and push changes so GitOps applies them:
+
[source,bash,options="wrap",role="execute"]
----
git add .
git commit -m "tenant(ai-agent): use my fork and correct webhook endpoint"
git push
----

== Ensure Secrets and Config

* Verify `agent-config` points to your Llama Stack service. Defaults to the in-cluster service:
+
`infra/tenants/ai-agent/base/sno/templates/configmap.yaml`
+
[source,yaml]
----
data:
  LLAMA_STACK_URL: "http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321"
  MODEL_ID: "llama-4-scout-17b-16e-w4a16"
  TEMPERATURE: "0.0"
  MAX_TOKENS: "5000"
----

* If needed for your Llama Stack config, hydrate any missing secrets (GitHub, Tavily) as per module 04/05.

== Build and Roll Out the App

* Trigger the build PipelineRun via the provided Job (creates a new run):
+
[source,bash,options="wrap",role="execute"]
----
oc -n ai-agent create -f infra/tenants/ai-agent/base/sno/templates/job.yaml
----

* Watch the PipelineRun in the OpenShift Console (Pipelines) until tasks complete.

* Verify the Deployment rolls out:
+
[source,bash,options="wrap",role="execute"]
----
oc -n ai-agent rollout status deploy/ai-agent
oc -n ai-agent get pods
----

* Get the route and test health:
+
[source,bash,options="wrap",role="execute"]
----
ROUTE=$(oc -n ai-agent get route ai-agent -o jsonpath='{.spec.host}')
curl -s http://${ROUTE}/health | cat
----

[TIP]
====
Reset/retry tips:

* If `pip install` upgraded dependencies earlier, restart the Jupyter kernel before re-running notebook cells (module 05)
* Stop previous port-forward (`Ctrl+C`) before starting a new one
* If a PipelineRun is stuck, cancel and start a fresh run; ensure Workspace PVC is recreated when using `generateName`
====

== Pipeline overview

This demo application pipeline models a common CI/CD flow and adds an automated recovery aid:

* Fetch source: `git-clone` pulls your repo and revision
* Build image: `buildah` builds and pushes the container image
* Deploy: `oc rollout status` verifies the deployment
* Finally: On failure, a `finally` step triggers the agent service with pod context

// Coming from xref:module-05.adoc[Module 05]: You codified the agent and validated the flow in a Workbench. Here we connect that flow to your pipeline so failures automatically notify the agent.

== Wire the Pipeline Trigger

The Tekton `agent-service-build` Pipeline includes a `finally` step that posts a failure payload to the agent service.

* Confirm the `finally` step now points to `/report-failure` and `namespace: ai-agent` in:
+
`infra/tenants/ai-agent/base/sno/templates/pipeline.yaml`

* Ensure the `ai-agent` service account/rolebinding exist in the `ai-agent` namespace:
+
[source,bash,options="wrap",role="execute"]
----
oc -n ai-agent get sa pipeline
oc -n ai-agent get rolebinding openshift-pipelines-edit
----

== End-to-End Test

To simulate a build failure and test the agent integration, use the pre-configured `bad` revision.

*Option 1: Web Console*

1. In the OpenShift Web Console, navigate to **Pipelines** in the `demo-pipeline` namespace.
2. Click on the `agent-service-build` pipeline.
3. Click **Start** to create a new PipelineRun.
4. In the parameters, set the `GIT_REVISION` value to `bad` to use the intentionally broken revision.
5. Start the PipelineRun.

*Option 2: Run a pre-created Job*

From the console, create a job from YAML using the `infra/tenants/demo-pipeline/base/sno/templates/job-bad.yaml` as a reference, or run via CLI:

[source,bash,options="wrap",role="execute"]
----
oc -n demo-pipeline create -f infra/tenants/demo-pipeline/base/sno/templates/job-bad.yaml
----

Feel free to use the web console or CLI to trigger the job.

*Option 3: One-off PipelineRun via CLI*

Start a one-off PipelineRun with the `bad` ref:

[source,bash,options="wrap",role="execute"]
----
oc -n demo-pipeline create -f - <<'EOF'
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: agent-service-build-run-bad-
  namespace: demo-pipieline
spec:
  taskRunTemplate:
    serviceAccountName: pipeline
  pipelineRef:
    name: agent-service-build
  params:
    - name: APP_NAME
      value: "ai-agent"
    - name: IMAGE_NAME
      value: "image-registry.openshift-image-registry.svc:5000/ai-agent/ai-agent"
    - name: GIT_REPO
      value: "https://github.com/${YOUR_GITHUB_USER}/etx-agentic-ai.git"
    - name: GIT_REVISION
      value: "bad"
    - name: PATH_CONTEXT
      value: "code"
  workspaces:
    - name: workspace
      volumeClaimTemplate:
        spec:
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 3Gi
EOF
----

* Intentionally break the build (e.g., temporarily change `code/Containerfile` to an invalid base) and trigger a build:
+
[source,bash,options="wrap",role="execute"]
----
oc -n ai-agent create -f infra/tenants/ai-agent/base/sno/templates/job.yaml
----

* When the PipelineRun fails, the `finally` step calls the agent. The agent reads logs via MCP OpenShift, searches the web, and creates a GitHub issue.

* Verify:
** Console → `ai-agent` → Workloads → Pods show agent logs with tool calls
** Your GitHub repo shows a new issue with the summary

== Artifacts to carry forward

* Route URL for `ai-agent` (health verified)
* Updated tenant pipeline/job YAML pointing to your fork
* A sample failed PipelineRun name and failing pod name
* URL of the created GitHub issue

== Troubleshooting

* If the PipelineRun cannot reach the agent route, verify the Route/Service are ready and DNS resolves in-cluster.
* If the agent times out on Llama Stack, confirm the server is healthy and reachable from the `ai-agent` namespace.
* If MCP tools are not registered, revisit module 04 to re-register `mcp::openshift` and `mcp::github` in Llama Stack.
* If no GitHub issue is created, check the agent logs for the tool call to `create_issue` and ensure the GitHub MCP server is configured.


== What’s Next

With the agent running as a service and integrated with the pipeline trigger, you have the foundation for a production rollout. You are now setup for a scenario where you have a new ticket coming in and you need to update your agent and your MTTR is fast due to the automation. Perhaps you are event adventurous enough to add the agent tooling itself as a `finally` call in your agent build pipeline to catch and resolve errors quickly. We will not be doing that today, so that will be left to the reader, but in the next module, we will discuss hardening, observability, and promotion flows.
== Next Steps

With your agent now running as a service and integrated with the pipeline trigger, you have established the foundation for a production-ready workflow. This setup enables rapid response to failures, as new issues are automatically created in your GitHub repository, reducing mean time to resolution (MTTR).

In the next module, you will learn how to further harden your deployment, add observability, and implement promotion flows to ensure your agent remains robust and reliable as you move toward production.

Onward to Module 07!
